Newtonian Program Analysis
Javier Esparza, Stefan Kiefer, and Michael Luttenberger
Institut f¨ ur Informatik, Technische Universit¨ at M¨ unchen, 85748 Garching, Germany
{esparza,kiefer,luttenbe }@model.in.tum.de
Abstract. This paper presents a novel generic technique for solving dataﬂow e quations in interproce-
dural dataﬂow analysis. The technique is obtained by generalizing New ton’s method for computing a
zero of a diﬀerentiable function to ω-continuous semirings. Complete semilattices, the common program
analysis framework, are a special class of ω-continuous semirings. We show that our generalized method
always converges to the solution, and requires at most as many iterations as current methods based
on Kleene’s ﬁxed-point theorem. We also show that, contrary to Kleen e’s method, Newton’s method
always terminates for arbitrary idempotent and commutative semirings . More precisely, in the latter
setting the number of iterations required to solve a system of nequations is at most n.
1 Introduction
This paper presents a novel generic technique for solving dataﬂow equations in interpr ocedural dataﬂow
analysis. It is obtained by generalizing Newton’s method, the 300-year-old techniq ue for computing a zero
of a diﬀerentiable function.
Our approach to interprocedural analysis is very similar to Sharir and Pnueli’s functional approach
[SP81,JM82,KS92,RHS95,SRH96,NNH99,RSJM05]. Sharir and Pnueli assume the following as given: a (join-)
semilattice1ofvalues , a mapping assigning to every program instruction a value, and a concatenation oper ator
that, given the values of two sequences of instructions, returns the value corresponding to their concatenation.
Sharir and Pnueli assume that the concatenation operator distributes over the latt ice’s join.2Sharir and
Pnueli deﬁne a system of abstract data ﬂow equations , containing one variable for each program point. They
show that for every procedure Pof the program and for every program point pofP, the least solution
of the system is the join of the values of all valid program paths starting at the initial node of Pand
leading to p. Sharir and Pnueli’s result was later extended by [KS92] to programs with local v ariables and
to non-distributive concatenation operators, which allows us to deal with certa in non-distributive analyses
[NNH99].
We slightly generalize Sharir and Pnueli’s setting. Loosely speaking, we al low to replace the join operator
with any operator satisfying the same algebraic properties with the po ssible exception of idempotence. In al-
gebraic terms, we extend the framework from lattices considered in [SP81] to ω-continuous semirings [Kui97],
an algebraic structure with two operations, usually called sum and product. The i nterest of this otherwise
simple extension is that our framework now encompasses equations over the semiring of the nonnegative
reals with addition and multiplication. This allows us to compare the eﬃciency of generic solution methods
for dataﬂow analysis when applied to the reals with the eﬃciency of methods supplied by cl assic numerical
mathematics, in particular Newton’s method.
It is well-known that Newton’s method, when it converges to a solution, usually con verges much faster
than classical ﬁxed-point iteration (see e.g. [OR70]). Furthermore, [EY09] have recently proved that Newton’s
method is guaranteed to converge for an analysis concerning the probability of ter mination of recursive
programs. These facts raise the question whether Newton’s method can be generalized to the more abstract
dataﬂow setting, where values are arbitrary entities, while preserving the goo d properties of Newton’s method.
1For reasons that will be clear later, we use join-semilattices rather t han meet-semilattices, deviating from the
classical dataﬂow analysis literature such as [Kil73,KU77,SP81]. As a con sequence, we also replace greatest ﬁxed
points by least ﬁxed points, meet-over-all-paths by join-over-all -paths, etc. This change is purely notational.
2Actually, in [SP81] the value of a program instruction is the function de scribing its eﬀect on program variables, and
the extension operator is function composition. However, the extens ion to an arbitrary distributive concatenation
operator is unproblematic.

In the ﬁrst part of the paper we show that the generalization is indeed possible. Inspi red by work of [HK99]
on Kleene algebras, we show that the notion of a diﬀerential of a function lying at the heart of Newton’s
method, and the method itself can be suitably generalized. This allows us to apply Newt on’s method to, for
instance, language equations. We then apply the method to two small case studies: a m ay-alias analysis and
an average runtime analysis.
In the second part of the paper we study the properties of Newton’s method on idempoten t semirings,
the classical domain of program analysis. Recall that the method is iterati ve: it constructs better and better
approximations to the solution of the equation system. We obtain a charact erization of the approximants,
and apply it to the case of commutative idempotent semirings, previously studied by Hopkins and Kozen in a
beautiful paper [HK99]. Hopkins and Kozen propose a generic solution method for the equations, and prove
that it terminates after O(3n) iterations, where nis the number of equations. We show that their method is
in fact Newton’s method, and, applying our characterization of the approximants, show that it terminates
after at most niterations.
Finally, in a short section we extend our framework to the non-distributive case. We show that Newton’s
method, like the classical ﬁxed-point iteration, computes an overapproximation of the join of the values of
all valid program paths.
In the rest of this introduction we go again through the paper’s skeleton sket ched above, but providing
some more details.
1.1 A Summary of Sharir and Pnueli’s Approach
[SP81] assume as given a lattice of data values with a join operator. They show how to compute for every
program point pof every procedure Pthe join of the values of all valid program paths leading from the
initial node of Ptop. This is called the join-over-all-valid-paths forp, or JOP( p) for short. The computation,
which works for distributive analyses, proceeds in two steps: ﬁrst, the join o ver all same-level valid program
paths is computed, where a path is same-level if every procedure call has a matching return. We denote this
join by JOP 0(p). The second step is usually described today in terms of summary edges (see e.g. [RHS95]).
JOP0(p) is used to construct a new ﬂowgraph without procedure calls. Edges calling Pare replaced by edges
with the same source and target nodes, but labelled with JOP 0(exP) (the eﬀect ofP) where exPis the exit
node of P; new edges are added leading from the source of each call to PtoP’s entry point. The result is
a ﬂowgraph without procedure calls, such that JOP( p) for the old and new graphs coincide. The JOP for
ﬂowgraphs without procedures (the intraprocedural case) is the least solution of a system of linear dataﬂow
equations [Kil73,KU77].
[SP81] show that JOP 0is equal to the least solution of a system of dataﬂow equations. We sketch how to
construct the equations by means of an example. Consider a program with three procedures X,Y,Z , whose
ﬂowgraphs are shown in Figure 1. Nodes correspond to program points, and edges to program instructions.
For instance, procedure Xcan execute band terminate, or execute a, call itself recursively, and, after the
call has terminated, call Y.
n1
n2
n3n5
n6 n7
n8 n9n11
n12
n13
n4 n10 n14X Y Z
a
b callX
callYcd
e callY callY
callZcallXg
i callX
h
Fig. 1. Flowgraphs of three procedures

To deﬁne the equations, Sharir and Pnueli assume a complete lattice3of values with a join operator ∨; a
mapping φassigning to each non-call edge ( m,n) a lattice value φ(m,n), and a concatenation operator ·that
distributes over ∨and has a neutral element 1. The system of equations contains a variable and an equat ion
for each program node. If nis the initial node of a procedure then it contributes the equation vn= 1, where
vndenotes n’s variable. Otherwise, it contributes the equation
vn=/logicalordisplay
m∈pred(n)vm·h(m,n)
where pred(n) denotes the set of immediate predecessors of n, and h(m,n) is deﬁned as follows: if ( m,n)
is a call edge calling, e.g., procedure X, then h(m,n) is the variable for the return node of X; otherwise
h(m,n) =φ(m,n).
The system of equations for Figure 1 can be more compactly represented if varia bles for all program
points other than return points are eliminated by substitution. Only three equat ions remain, namely those
for the return points n4,n10, and n14. If moreover, and abusing language, we reuse X,Y,Z to denote the
variables for these points, and a,... ,i to denote the values φ(n1,n2),... ,φ (n11,n14), we obtain the system
X=a·X·Y∨b
Y=c·Y·Z∨d·Y·X∨e (1)
Z=g·X·h∨i
which very closely resembles the structure of the ﬂowgraphs. Since the right-hand sides o f the equations are
monotonic mappings, and ·distributes over ∨, the existence of the least ﬁxed point is guaranteed by Kleene’s
ﬁxed-point theorem.
1.2 A Slight Generalization: From Semilattices to Semirings
Let us examine the properties of the join operator ∨. First of all, since the lattice is complete, it is deﬁned for
arbitrary sets of lattice elements. Furthermore, it is associative, commut ative, idempotent, and concatenation
distributes over it. If we use the symbols 0 for the bottom element of the lat tice (corresponding to an abort
operation) and 1 for the element corresponding to a NOP instruction, then we have 0 ∨a=a∨0 =aand
1·a=a·1 =afor every a. It is argued in [SF00] that one can transform every program analysis to an
essentially equivalent one that satisﬁes 0 ·a=a·0 = 0. So the lattice, together with the two operations ∨
and·and the elements 0 and 1, constitutes an idempotent semiring . In the following we write ‘+’ for ‘ ∨’ to
conform with the standard semiring notation.
Idempotence of the join operator is not crucial for the existence of the least ﬁxed po int; it can be
replaced by a weaker property. Consider the relation ⊑on semiring elements deﬁned as follows: a⊑a+b
for all elements a,b. A semiring is naturally ordered if this relation is a partial order, and a naturally ordered
semiring in which inﬁnite sums exist and satisfy standard properties is called ω-continuous . Using Kleene’s
ﬁxed-point theorem it is easy to show that systems of equations over ω-continuous semirings still have a
least ﬁxed point with respect to the partial order ⊑(see for instance [Kui97]).
As an example of application of this more general setting, assume that the prog ram of Figure 1 is proba-
bilistic, and the values a,... ,i are real numbers corresponding to the probabilities of taking the transitions.
A particular case is shown in Figure 2. The semiring operations are addition and multiplication over the
nonnegative reals. Notice that addition is not idempotent. The semiring is ω-continuous if a new element ∞
with the usual properties is added. It is not diﬃcult to show [EKM04,EY09] that the least solution of the
system
X= 0.4XY+ 0.6
3More precisely, [SP81] initially consider semilattices with a leas t and a greatest element that satisfy the ascending-
chain property (every non-decreasing chain eventually becomes stat ionary). However, the paper later concentrates
on ﬁnite lattices, which are complete.

n1
n2
n3n5
n6 n7
n8 n9n11
n12
n13
n4 n10 n14X Y Z
0.4
0.6 callX
callY0.30.4
0.3 callY callY
callZcallX0.3
0.7 callX
1
Fig. 2. Probabilistic ﬂowgraphs
Y= 0.3Y Z+ 0.4Y X+ 0.3
Z= 0.3X+ 0.7
yields the probability of termination of each procedure. (Incidentally, notice that , contrary to the intraproce-
dural case, this probability may be diﬀerent from 1 even if every execution can be extended to a terminating
execution.)
1.3 Solving Systems of Equations
Current generic algorithms for solving Sharir and Pnueli’s equations (like t he classical worklist algorithm of
dataﬂow analysis) are based on variants of Kleene’s ﬁxed-point theorem [Kui97]. T he theorem states that
the least solution µfof a system of equations X=f(X) over an ω-continuous semiring is equal to the
supremum of the sequence ( κ(i))i∈NofKleene approximants given by κ(0)=0(the vector of 0-elements) and
κ(i+1)=f(κ(i)). This yields a procedure (let us call it Kleene’s method ) to compute or at least approximate
µf. If the domain satisﬁes the well-known ascending chain condition [NNH99], then the procedure terminates,
because there exists an isuch that κ(i)=κ(i+1)=µf.
Kleene’s method is generic and robust: it always converges when started at 0, for any ω-continuous
semiring and for any system of equations. On the other hand, it often fails to ter minate, and it can converge
very slowly to the solution. We illustrate this point by means of two si mple examples. Consider the equation
X=a·X+bover the lattice of subsets of the language {a,b}∗. The least solution is the regular language
a∗b, but we have κ(i)={b,ab,... ,ai−1b}fori≥1, i.e., the solution is not reached in any ﬁnite number of
steps. For our second example consider a very simple probabilistic procedure that can eit her terminate or
call itself twice, both with probability 1 /2. The probability of termination of this program is given by the
least solution of the equation X= 1/2 + 1/2·X2(where X2abbreviates X·X). It is easy to see that the
least solution is equal to 1, but we have κ(i)≤1−1
i+1for every i≥0, i.e., in order to approximate the
solution within ibits of precision we have to compute about 2iKleene approximants. For instance, we have
κ(200)= 0.9990, i.e., 200 iterations produce only three digits of precision.
After our slight generalization of Sharir and Pnueli’s framework, quantit ative analyses like the probability
of termination fall within the scope of the approach. So we can look at numeri cal mathematics for help with
the ineﬃciencies of Kleene’s method.
As could be expected, faster approximation techniques for equations over the reals ha ve been known
for a long time. In particular, Newton’s method, suggested by Isaac Newton mo re than 300 years ago, is a
standard eﬃcient technique to approximate a zero of a diﬀerentiable function, and can b e adapted to our
problem. Since the least solution of X= 1/2 + 1/2·X2is a zero of 1 /2 + 1/2·X2−X, the method can
be applied, and it yields ν(i)= 1−2−ifor the i-thNewton approximant . So the i-th Newton approximant
already has ibits of precision, instead of log ibits for the Kleene approximant.
However, Newton’s method also has a number of disadvantages, at least at ﬁrst sight. Newton’s method
on the real ﬁeld is by far not as robust and well behaved as Kleene’s method on semirings . The method may
converge very slowly, converge only locally (only when started in a small neighborhood of the zero), or even

not converge at all [OR70]. So we face the following situation. Kleene’s method, a robust and general solution
technique for arbitrary ω-continuous semirings, is ineﬃcient in many cases. Newton’s method is usually ver y
eﬃcient, but it is only deﬁned for the real ﬁeld, and it is not robust.
As part of their study of Recursive Markov Chains, [EY09] showed that a varia nt of Newton’s method
is robust for certain systems of equations over the real semiring : the method always converges when started
at zero. In other words, moving from the real ﬁeld to the real semiring (only nonnega tive numbers) makes
the instability problems disappear. Inspired by this work, in this paper we obta in a more general result. We
show that Newton’s method can be generalized to arbitrary ω-continuous semirings, and prove that on these
structures it is as robust as Kleene’s method. Since lattices, the classical domain of pr ogram analysis, are
very close to idempotent semirings, we study in detail Newton’s method in idempo tent semirings. We pay
special attention to idempotent semirings with commutative multiplicati on. Loosely speaking, these semirings
correspond to counting analysis , in which one is interested in how often program points are visited, but not i n
which order. These semirings do not always satisfy the ascending chain condition, a nd Kleene’s method may
not terminate. We show that a very elegant iterative solution method for these semirings due to [HK99], is
exactly Newton’s method, and always terminates in a ﬁnite number of steps. As ment ioned above, we further
use our characterization of Newton approximants to show that the least ﬁxed po int is reached after at most
niterations, a tight bound, improving on the O(3n) bound of [HK99].
The paper is divided into two parts. The ﬁrst part introduces our generalization of Newton’s method,
and ends with two examples of application to program analysis problems: a may-a lias analysis for a program
transforming a tree into a list, and an average runtime analysis for l azy evaluation of And/Or-trees. The
second part presents the proofs of our results, investigates Newton’s method in i dempotent and commutative
semirings, and extends our approach to semi-distributive program analyses. It i s well-known that in this case
ﬁxed-point iteration overapproximates the join-over-all-paths value (see e.g. [ KS92,RHS95,SRH96,NNH99]).
We show that the same property holds for Newton’s method.
The ﬁrst part of the paper is organized as follows. Section 2 introduces ω-continuous semirings, systems
of ﬁxed-point equations, and some semirings investigated in the rest of the pap er. Section 3 recalls Newton’s
method, and generalizes it to arbitrary ω-continuous semirings. Section 4 presents the case studies. The
second part starts with Section 5 where we prove the fundamental properties of our g eneralization, mainly
convergence to the least ﬁxed point. Section 6 characterizes the Newton approximant s in terms of derivation
trees, a generalization of the derivation trees of language theory. Section 7 uses this c haracterization to prove
that for idempotent and commutative semirings Newton’s method always termi nates in at most niterations
for a system of dimension n. Finally, Section 8 deals with non-distributive program analyses.
2ω-Continuous Semirings
Deﬁnition 2.1. Asemiring is a tuple /a\}bracke⊔le{⊔S,+,·,0,1/a\}bracke⊔ri}h⊔where Sis a set containing two distinguished elements
0and1, and the binary operations +,·:S×S→Ssatisfy the following conditions:
(1)/a\}bracke⊔le{⊔S,+,0/a\}bracke⊔ri}h⊔is a commutative monoid.
(2)/a\}bracke⊔le{⊔S,·,1/a\}bracke⊔ri}h⊔is a monoid.
(3)0·a=a·0 = 0 for all a∈S.
(4)a·(b+c) =a·b+a·cand(a+b)·c=a·c+b·cfor all a,b,c ∈S.
A semiring /a\}bracke⊔le{⊔S,+,·,0,1/a\}bracke⊔ri}h⊔isω-continuous if the following additional conditions hold:
(5) The relation ⊑:={(a,b)∈S×S| ∃d∈S:a+d=b}is a partial order.
(6) Every ω-chain ( ai)i∈N(i.e.ai⊑ai+1withai∈S) has a supremum w.r.t. ⊑denoted by supi∈Nai.
(7) Given an arbitrary sequence (bi)i∈N, deﬁne
/summationdisplay
i∈Nbi:= sup {b0+b1+...+bi|i∈N}
(the supremum exists by condition (6)). For every sequence (ai)i∈N, for every c∈S, and for every
partition (Ij)j∈JofN:
c·/parenleftBigg/summationdisplay
i∈Nai/parenrightBigg
=/summationdisplay
i∈N(c·ai),/parenleftBigg/summationdisplay
i∈Nai/parenrightBigg
·c=/summationdisplay
i∈N(ai·c),/summationdisplay
j∈J
/summationdisplay
i∈Ijaj
=/summationdisplay
i∈Nai.

An (ω-continuous) semiring is idempotent , ifa+a=aholds for all a∈S. It is commutative , ifa·b=b·a
for all a,b∈S. In an ω-continuous semiring we deﬁne the Kleene-star∗:S→Sby
a∗:=/summationdisplay
k∈Nak= sup {1 +a+a·a+...+ak|k∈N}fora∈S.
Forω-continuous semirings, we have the following important property that addit ion and multiplication,
and subsequently polynomials are ω-continuous, too.
Lemma 2.2. In any ω-continuous semiring /a\}bracke⊔le{⊔S,+,·,0,1/a\}bracke⊔ri}h⊔addition and multiplication are ω-continuous, i.e.
for any ω-chain (ai)i∈Nand any c∈Swe have
c·(sup
i∈Nai) = sup
i∈N(c·ai),(sup
i∈Nai)·c= sup
i∈N(ai·c), c+ (sup
i∈Nai) = sup
i∈N(c+ai).
Proof. By (5) and (6) in the deﬁnition above, for any ω-chain ( ai)i∈N, there exists a sequence ( di)i∈Nsuch
thatd0=a0andai+di=ai+1(i.e.diisa diﬀerence ofai+1andai), and so supi∈Nai=/summationtext
i∈Ndi. The result
follows by applying (7) to this sequence.
Example 2.3. Common examples of ω-continuous semirings are the real semiring , i.e., nonnegative real num-
bers extended by inﬁnity /a\}bracke⊔le{⊔R≥0∪{∞} ,+,·,0,1/a\}bracke⊔ri}h⊔, and the language semiring over some ﬁnite alphabet Σ, i.e.,
/a\}bracke⊔le{⊔2Σ∗,∪,·,∅,{ε}/a\}bracke⊔ri}h⊔where ·stands for the canonical concatenation of languages, and εfor the empty word. In
both of these instances the natural order coincides with the canonical order on the respecti ve carrier, i.e., in
the real semiring we have ⊑ ≡ ≤ , and in the language semiring ⊑ ≡ ⊆ .
In the following we often write abinstead of a·b.
2.1 Vectors, Polynomials and Power Series.
LetSbe an ω-continuous semiring and let Xbe a ﬁnite set of variables. A vector is a mapping v:X →S
which assigns every variable X∈ Xthe value v(X). We usually write vXforv(X). If there is some natural
total order given on Xlike e.g. the lexicographic order in the case X={X,Y,Z }or the total order on
the indices in the case X={X1,X2,X3}we will also write a vector vas a column vector of dimension
|X|enumerating the values starting with the lowest variable as the topmost val ue. The set of all vectors is
denoted by V.
Given a countable set Iand a vector vifor every i∈I, we denote by/summationtext
i∈Ivithe vector given by/parenleftbig/summationtext
i∈Ivi/parenrightbig
X=/summationtext
i∈I(vi)Xfor every X∈ X. Throughout the paper we use bold letters like ‘ v’ or ‘a’ for
vectors.
Amonomial is a ﬁnite expression a1X1a2X2· · ·akXkak+1, where k≥0,a1,... ,a k+1∈Sand
X1,... ,X k∈ X. Note that this general deﬁnition of monomial is necessary as we do not require that
multiplication is commutative. A polynomial is an expression of the form m1+...+mkwhere k≥0 and
m1,... ,m kare monomials. A power series is an expression of the form/summationtext
i∈Imi, where Iis a countable set
andmiis a monomial for every i∈I.
Given a monomial f=a1X1a2X2... a kXkak+1and a vector v, we deﬁne f(v), the value of fatv, as
a1vX1a2vX2· · ·akvXkak+1. We extend this to any power series f=/summationtext
i∈Ifibyf(v) =/summationtext
i∈Ifi(v).
Avector of power series is a mapping fthat assigns to each variable X∈ Xa power series f(X). Again
we write fXforf(X). Given a vector v, we deﬁne f(v) as the vector satisfying ( f(v))X=fX(v) for every
X∈ X, i.e., f(v) is the vector that assigns to Xthe result of evaluating the power series fXatv. So,f
naturally induces a mapping f:V→V.
2.2 Fixed-Point Equations and Kleene’s Theorem.
The partial order ⊑on the semiring Scan be lifted to a partial order on vectors, also denoted by ⊑, and
deﬁned by v⊑v′ifvX⊑v′
Xfor every X∈ X.
Given a vector of power series f, we are interested in the least ﬁxed point of f, i.e., the least vector v
w.r.t. ⊑satisfying v=f(v). We brieﬂy recall Kleene’s theorem, which guarantees that the least ﬁxed point
exists.

A mapping f:S → S ismonotone ifa⊑bimplies f(a)⊑f(b), and ω-continuous if for any inﬁnite
chain a0⊑a1⊑a2⊑...we have sup {f(ai)}=f(sup{ai}). These deﬁnitions are extended to mappings
f:V→Vfrom vectors to vectors by requiring them to hold in every component of f. The following result is
taken from [Kui97] and relies on the fact that multiplication and addition are ω-continuous on ω-continuous
semirings, see Lemma 2.2.
Proposition 2.4. Letfbe a vector of power series. The mapping induced by fis monotone and ω-
continuous. Hence, by Kleene’s theorem, fhas a unique least ﬁxed point µf. Further, µfis the supremum
(w.r.t. ⊑) of the Kleene sequence given by κ(0)=f(0), and κ(i+1)=f(κ(i)).4
2.3 Some Semiring Interpretations.
We recall that diﬀerent interesting pieces of information about the program of F igure 1 correspond to the least
solution of Equations (1) from page 3 over diﬀerent semirings.5For the rest of the section let Σ={a,b,... ,i }
be the set of actions in the program, and let σdenote an arbitrary element of Σ.
Language interpretation Consider the following semiring. The carrier is 2Σ∗(i.e., the set of languages
overΣ). The semiring element σis interpreted as the singleton language {σ}. The sum and product operations
are union and concatenation of languages, respectively. We call this structure language semiring overΣ.
Under this interpretation, Equations (1) are nothing but the following context-fr ee grammar in Backus-Naur
form:
X→aXY |b Y →cY Z|dY X |e Z →gXh|i
The least solution of (1) is the triple ( L(X),L(Y),L(Z)), where, for U∈ {X,Y,Z },L(U) denotes the set
of terminating executions of the program with Uas main procedure, or, in language-theoretic terms, the
language of the associated grammar with Uas axiom.
Relational interpretation Assume that an action σcorresponds to a program instruction whose semantics
is described by means of a relation Rσ(V,V′) over a set Vof program variables (as usual, primed and
unprimed variables correspond to the values before and after executing the instruction) . Consider now the
following semiring. The carrier is the set of all relations over ( V,V′). The semiring element σis interpreted
as the relation Rσ. The sum and product operations are union and join of relations, respectively, i. e.,
(R1·R2)(V,V′) =∃V′′R1(V,V′′)∧R2(V′′,V′). Under this interpretation, the U-component of the least
solution of (1) is the summary relation RU(V,V′) containing the pairs V,V′such that if procedure Ustarts
at valuation V, then it may terminate at valuation V′.
Counting interpretation Assume we wish to know how many as,bs, etc. we can observe in a (terminating)
execution of the program, but we are not interested in the order in which they occur. In the terminology of
abstract interpretation, we abstract an execution w∈Σ∗by the vector ( na,... ,n i)∈N|Σ|where na,... ,n i
are the number of occurrences of a,... ,i inw. We call ( na,... ,n i) theParikh image ofw. The Parikh images
ofL(X),L(Y),L(Z) are the least solution of (1) for the following semiring. The carrier is 2N|Σ|. The j-th
action of Σis interpreted as the singleton set {(0,... ,0,1,0... ,0)}with the “1” at the j-th position. The
sum operation is set union, and the product operation is given by
S·T={(sa+ta,... ,s i+ti)|(sa,... ,s i)∈S,(ta,... ,t i)∈T}.
4Deﬁning κ(0)=0would be more straightforward, but less convenient for this paper.
5This will be no surprise for the reader acquainted with abstract int erpretation, but the examples will be used all
throughout the paper.

Probabilistic interpretations Assume that the choices between actions are stochastic. For instance,
actions aandbare chosen with probability pand (1 −p), respectively. The probability of termination is given
by the least solution of (1) when interpreted over the following semiring (the real semiring ) [EKM04,EY09].
The carrier is the set of nonnegative real numbers, enriched with an additional elemen t∞. The semiring
element σis interpreted as the probability of choosing σamong all enabled actions. Sum and product are
the standard operations on real numbers, suitably extended to ∞.
Assume now that actions are assigned not only a probability, but also a duration . Let dσdenote the
duration of σ. We are interested in the expected termination time of the program, under the condit ion that
the program terminates (the conditional expected time ). For this we consider the following semiring. The
elements are the set of pairs ( r1,r2), where r1,r2are nonnegative reals or ∞. We interpret σas the pair
(pσ,dσ), i.e., the probability and the duration of σ. The sum operation is deﬁned as follows (where to simplify
the notation we use + eand·efor the operations of the semiring, and + and ·for sum and product of reals):
(p1,d1) +e(p2,d2) =/parenleftbigg
p1+p2,p1·d1+p2·d2
p1+p2/parenrightbigg
(p1,d1)·e(p2,d2) = (p1·p2,d1+d2)
The reader can easily check that this deﬁnition satisﬁes the semiring axioms. The U-component of the least
solution of (1) is now the pair ( tU,eU), where tUis the probability that procedure Uterminates, and eUis
its conditional expected time.
3 Newton’s Method for ω-Continuous Semirings
We introduce our generalization of Newton’s method for ω-continuous semirings. In Section 3.1 we consider
the univariate case, i.e. the case of one equation in a single variable, which a lready allows us to introduce
all important ideas. Here we ﬁrst recall Newton’s method as known from calcul us, i.e., as a method for
approximating a zero of a diﬀerentiable function. We then take a close look at the analytical deﬁnition,
and identify the obstacles for a generalization to ω-continuous semirings. Finally, we propose a deﬁnition
that overcomes the obstacles. In Section 3.2 we turn to the multivariate case a nd state a fundamental
theorem which shows that our generalization of Newton’s method is well-deﬁned and conver ges to the least
ﬁxed point. This lays the foundation to what we call Newtonian program analysis , the application of the
generalized version of Newton’s method to program analysis. We illustrate the concepts at the end of this
section.
3.1 The Univariate Case
Given a diﬀerentiable function g:R→R, Newton’s method computes a zero of g, i.e., a solution of the
equation g(X) = 0. The method starts at some value ν(0)“close enough” to the zero, and proceeds iteratively:
given ν(i), it computes a value ν(i+1)closer to the zero than ν(i). For that, the method linearizes gatν(i), i.e.,
computes the tangent to gpassing through the point ( ν(i),g(ν(i))), and takes ν(i+1)as the zero of the tangent
(i.e., the x-coordinate of the point at which the tangent cuts the x-axis), see Figure 3 for an illustration.
It is convenient for our purposes to formulate Newton’s method in terms of the diﬀerential ofgat a given
point v∈R. Recall that the diﬀerential of gis the mapping Dg|v:R→Rthat assigns to each v∈Rthe
linear function describing the tangent of gat the point ( v,g(v)) in the coordinate system having ( v,g(v))
as origin. If we denote the diﬀerential of gatvbyDg|v, then we have Dg|v(X) =g′(v)·X(for example,
ifg(X) =X2+ 3X+ 1, then Dg|3(X) = 9X). In terms of diﬀerentials, Newton’s method is formulated as
follows. Starting at some ν(0), compute iteratively ν(i+1)=ν(i)+∆(i), where ∆(i)is the solution of the linear
equation Dg|ν(i)(X) +g(ν(i)) = 0 (assume for simplicity that the solution of the linear system is unique). In
particular for a univariate function gon the real numbers we obtain for ∆(i)
0 =Dg|ν(i)(∆(i)) +g(ν(i)) =g′(ν(i))·∆(i)+g(ν(i)),i.e.,∆(i),=−g(ν(i))
g′(ν(i))

0.20.2
0.40.4
0.60.6
0.80.8
11
ν(0)ν(1)ν(2)g(X)
Fig. 3. Newton’s method to ﬁnd a zero of a one-dimensional function g(X).
and, thus, the standard formulation of Newton’s method:
ν(i+1)=ν(i)+∆(i)=ν(i)−g(ν(i))
g′(ν(i)).
Computing the solution of a ﬁxed-point equation, f(X) =Xamounts to computing a zero of g(X) =
f(X)−X, and so we can apply Newton’s method. Since for every real number vwe have Dg|v(X) =
Df|v(X)−X, the method looks as follows:
Starting at some ν(0), compute iteratively
ν(i+1)=ν(i)+∆(i)(2)
where ∆(i)is the solution of the linear equation
Df|ν(i)(X) +f(ν(i))−ν(i)=X . (3)
So Newton’s method “breaks down” the problem of ﬁnding a solution to a non-linear system f(X) =X
into ﬁnding solutions to the sequence (3) of linear systems.
Generalization Generalizing Newton’s method to arbitrary ω-continuous semirings requires us to overcome
two obstacles. First, the notion of diﬀerential seems to require a richer alg ebraic structure than a semiring:
diﬀerentials are usually deﬁned in terms of derivatives, which are the limit of a quotient of diﬀerences, which
requires both the sum and product operations to have inverses. Second, equation (3) con tains the term
f(ν(i))−ν(i), which again seems to be deﬁned only if summation has an inverse.
The ﬁrst obstacle Diﬀerentiable functions satisfy well known algebraic rules with respect to sum s and prod-
ucts of functions. We take these rules as the deﬁnition of the diﬀerential of a power s eriesfover an ω-
continuous semiring S. We remark that this deﬁnition of diﬀerential generalizes the usual algebraic deﬁnitio n
of derivatives.
Deﬁnition 3.1. Letfbe a power series in one variable Xover an ω-continuous semiring S. The diﬀerential
offat the point vis the mapping D f|v:S→Sinductively deﬁned as follows for every b∈S:
Df|v(b) =

0 iff∈S
b iff=X
Dg|v(b)·h(v) +g(v)·Dh|v(b)iff=g·h/summationtext
i∈IDfi|v(b) iff=/summationtext
i∈Ifi(b).

Example 3.2. First consider a polynomial fover some commutative ω-continuous semiring. Because of
commutative multiplication, we may write any monomial as a·Xkfor some k∈Nanda∈S, and so
f=/summationtextn
k=0ak·Xkfor suitable n∈Nandak∈S. Letf′denote the usual algebraic derivative of fw.r.t. X,
i.e.f′=/summationtextn
k=1k·ak·Xk−1where k·akis an abbreviation of/summationtextk
i=1ak. We then have
Df|v(b) =/summationtextn
k=0D(ak·Xk)|v(b)
=/summationtextn
k=0(Dak|v(b)·(Xk)(v) +/summationtextk−1
j=0ak·(Xj)(v)·DX|v(b)·(Xk−1−j)(v))
=/summationtextn
k=0/summationtextk−1
j=0ak·vj·DX|v(b)·vk−1−j
= (/summationtextn
k=1k·ak·vk−1)·b
=f′(v)·b.
So, on commutative semirings, we have Df|v(b) =f′(v)·bfor all v,b∈S.
Now, assume that multiplication is not commutative, and consider the simple cas e of a quadratic mono-
mialm=a0Xa1Xa2. We then have
Dm|v(b) =a0·DX|v(b)·a1·v·a2+a0·v·a1·DX|v(b)·a2
=a0·b·a1·v·a2+a0·v·a1·b·a2.
The important point here is that the diﬀerential “remembers” the position of the variables, and therefore
does not simply append the value b. ⊓ ⊔
Remark 3.3. LetΣbe a ﬁnite alphabet, L⊆Σ∗a language and u∈Σ∗a ﬁnite word. In [Brz64] the derivative
DuLofLw.r.t. uis deﬁned to be the language {w|uw∈L}. One may relate this notion of derivative to
our deﬁnition of diﬀerential for the special case of univariate power series on i dempotent and commutative
semirings. For instance, writing the power series f(X) =a+Xb+XXc as the language Lf:={a,Xb,XXc }
(with a,b,c,X ∈Σ), its derivative w.r.t. XisDXLf={b,Xc}. Writing this language as power series
g(X) =b+Xc, we see that g(X) is related to the diﬀerential DfbyDf|v(e) =be+vce=g(v)·ein this
case. If multiplication is notcommutative, then Df|v(e) =eb+evc+vec, so the equality Df|v(e) =g(v)·e
no longer holds.
The second obstacle Proﬁting from the fact that 0 is the unique minimal element of Swith respect to ⊑,
we ﬁx ν(0)=f(0), which guarantees ν(0)⊑f(ν(0)). We guess that with this choice ν(i)⊑f(ν(i)) will hold
not only for i= 0, but for every i≥0 (the correctness of this guess is proved in Theorem 3.9). If the guess
is correct, then, by the deﬁnition of ⊑, the semiring contains an element δ(i)such that f(ν(i)) =ν(i)+δ(i).
We replace f(ν(i))−ν(i)by any such δ(i). This leads to the following deﬁnition:
Deﬁnition 3.4. Letfbe a power series in one variable. A Newton sequence ( ν(i))i∈Nis given by:
ν(0)=f(0)and ν(i+1)=ν(i)+∆(i)(4)
where ∆(i)is the least solution of
Df|ν(i)(X) +δ(i)=X (5)
andδ(i)is any element satisfying f(ν(i)) =ν(i)+δ(i).
Theorem 3.9 below shows that Newton sequences always exist (i.e., there is alway s at least one possible
choice for δ(i)), and that they all converge at least as fast as the Kleene sequence. More precisely, we show
that for every i≥0
κ(i)⊑ν(i)⊑ν(i+1)⊑µf .
Since we have µf= supi∈Nκ(i)by Kleene’s theorem, Newton sequences converge to µf.
In general, there can be more than one choice for δ(i). But Theorem 3.9 also shows that the Newton
sequence ( ν(i))i≥0itself is uniquely determined by f(andS). In other words, the choice of δ(i)does not
inﬂuence the Newton approximants ν(i).
Let us consider some examples for Newton sequences.

Examples We compute the Newton sequence for a program that can execute aand terminate, or execute b
and then call itself twice, recursively (the abstract scheme of a divide-and-conquer procedure). The abstract
equation of the program is
X=a+b·X·X (6)
The real semiring Consider the case a=b= 1/2 (we can interpret aandbas probabilities). We have
Df|v(X) =v·X, and one single possible choice for δ(i), namely δ(i)=f(ν(i))−ν(i)= 1/2+1/2(ν(i))2−ν(i).
Equation (5) becomes
ν(i)X+ 1/2 + 1/2(ν(i))2−ν(i)=X
with∆(i)= (1−ν(i))/2 as unique solution. We get
ν(0)= 1/2 ν(i+1)= (1 + ν(i))/2
and therefore ν(i)= 1−2(i+1). So the Newton sequence converges to 1, and gains one bit of accuracy per
iteration.
The language semiring Consider the language semiring with Σ={a,b}. The product operation is con-
catenation of languages, and hence non-commutative. So we have Df|v(X) = bvX+bXv. We show in
Proposition 7.1 that when sum is idempotent (as in this case, where it is union of languages) the deﬁnition
of the Newton sequence can be simpliﬁed to
ν(0)=f(0) and ν(i+1)=∆(i), (7)
where ∆(i)is the least solution of
Df|ν(i)(X) +f(ν(i)) =X . (8)
With f=a+b·X·Xfrom Equation (6), Equation (8) becomes
bν(i)X+bXν(i)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Df|ν(i)(X)+a+bν(i)ν(i)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
f(ν(i))=X . (9)
Its least solution (which by (7) is equal to the ( i+1)-st Newton approximant) is a context-free language. Let
G(i)be a grammar with axiom S(i)such that ν(i)=L(G(i)). Since ν(0)=f(0), the grammar G(0)contains
one single production, namely S(0)→a. Equation (9) allows us to deﬁne G(i+1)in terms of G(i), and we get:
G(0)={S(0)→a}
G(i+1)=G(i)∪ {S(i+1)→a|bS(i+1)S(i)|bS(i)S(i+1)|bS(i)S(i)}
The counting semiring Consider the counting semiring with ra={(1,0)}andrb={(0,1)}. Since the sum
operation is union of sets of vectors, it is idempotent and Equations (7) and (8) hold. Since the product
operation is now commutative, we obtain for our example
b·ν(i)·X+a+b·ν(i)·ν(i)=X (10)
Using Kleene’s ﬁxed-point theorem (Proposition 2.4), it is easy to see that the l east solution of a linear
equation X=u·X+vover a commutative ω-continuous semiring is u∗·v, where u∗=/summationtext
i∈Nui. The least
solution ∆(i)of Equation (10) is then given by
∆(i)= (rb·ν(i))∗·(ra+rb·ν(i)·ν(i))
and we obtain:
ν(0)=ra={(1,0)}
ν(1)= (rb·ra)∗·(ra+rb·ra·ra) ={(n,n)|n≥0} · {(1,0),(2,1)}
={(n+ 1,n)|n≥0}
ν(2)= ({(n,n)|n≥1})∗·({(1,0)} ∪ {(2n+ 2,2n+ 1)|n≥0})
={(n+ 1,n)|n≥0}
So the Newton sequence reaches a ﬁxed point after one iteration. In Section 7 we show tha t the Newton
sequence of a system of nequations over any commutative andidempotent semiring converges after at most n
iterations. Further note that the counting semiring does not satisfy the ascending-cha in property, i.e., there
are monotonically increasing sequences in the counting semiring which do not become sta tionary. Therefore,
the Kleene sequence and its variations do not reach µfafter a ﬁnite number of steps in general.

3.2 The Multivariate Case
Newton’s method can be easily generalized to the multivariate case. Given diﬀeren tiable functions
g1,... ,g n:Rn→R, the method computes a solution of g(X) =0, where g= (g1,... ,g n); starting at
some ν(0), the method computes ν(i+1)=ν(i)+∆(i), where ∆(i)is the solution of the system of linear
equations
Dg1|ν(i)(X) +g1(ν(i)) = 0
...
Dgn|ν(i)(X) +gn(ν(i)) = 0
andDgj|ν(i)(X) is the diﬀerential of gjatν(i), i.e., the function corresponding to the tangent hyperplane
ofgjat the point ( ν(i),gj(ν(i)).
Given a function g:Rn→Rdiﬀerentiable at a point v, there exists a function DXg|vfor each variable
X∈ Xsuch that Dg|v=/summationtext
X∈XDXg|v. These functions are closely related to the partial derivatives, more
precisely we have DXg|v(X) =∂g
∂X/vextendsingle/vextendsingle/vextendsingle
v·X.
We denote the system above by Dg|ν(i)(X) +g(ν(i)) =0. For the problem of computing a solution of a
system of ﬁxed-point equations, the method looks as follows:
starting at some ν(0), compute iteratively
ν(i+1)=ν(i)+∆(i)(11)
where ∆(i)is the least solution of the linear system of ﬁxed-point equations
Df|ν(i)(X) +f(ν(i))−ν(i)=X. (12)
Generalization Again, we use the algebraic deﬁnition of diﬀerential:
Deﬁnition 3.5. Letfbe a power series over an ω-continuous semiring Sand let X∈ Xbe a variable. The
diﬀerential of fw.r.t. Xat the point vis the mapping D Xf|v:V→Sinductively deﬁned as follows:
DXf|v(b) =

0 iff∈Sorf∈ X \ { X}
bX iff=X
DXg|v(b)·h(v) +g(v)·DXh|v(b)iff=g·h/summationtext
i∈IDXfi|v(b) iff=/summationtext
i∈Ifi.
Further, we deﬁne the diﬀerential of fatvas the function
Df|v:=/summationdisplay
X∈XDXf|v.
Finally, the diﬀerential of a vector of power series fatvis deﬁned as the function D f|v:V→Vwith
(Df|v(b))X:=DfX|v(b).
As in the univariate case we guess that ν(i)⊑f(ν(i)) will hold for every i≥0. If the guess is correct,
then the semiring contains an element δ(i)such that f(ν(i)) =ν(i)+δ(i), and Equation (12) becomes
Df|ν(i)(X) +δ(i)=X. (13)
This leads to the following deﬁnition:
Deﬁnition 3.6. Letf:V→Vbe a vector of power series.

–Leti∈N. Ani-thNewton approximant ν(i)is inductively deﬁned by
ν(0)=f(0)and ν(i+1)=ν(i)+∆(i),
where ∆(i)is the least solution of Equation (13)andδ(i)is any vector satisfying f(ν(i)) =ν(i)+δ(i).
–A sequence (ν(i))i∈Nof Newton approximants is called Newton sequence .
Remark 3.7. One can easily show by induction that for any v,b,b′∈V, and any vector of power series f
we have
Df|v(b+b′) =Df|v(b) +Df|v(b′).
Remark 3.8. If the product operation of the semiring is commutative, the diﬀerential DXf|v(a) can be
written as∂f
∂X|v·aX, where∂f
∂X|vdenotes the usual partial derivative of the power series fw.r.t. X, taken
atv, as known from algebra:
∂f
∂X/vextendsingle/vextendsingle/vextendsingle/vextendsinglev=

0 if f∈Sorf∈ X \ { X}
1 if f=X
∂g
∂x|v·h(v) +g(v)·∂h
∂X|viff=g·h/summationtext
i∈I∂fi
∂X|v iff=/summationtext
i∈Ifi.
So, in commutative semirings we may use the usual representation of the diﬀerent ial by means of the gradient
of a power series f, or more generally, by the Jacobian of a vector fof power series.
The following fundamental theorem shows that there exists exactly one Newton sequence, that it converges
to the least ﬁxed point, and that it does so at least as fast as the Kleene sequence.
Theorem 3.9. Letf:V→Vbe a vector of power series.
–There is exactly one Newton sequence (ν(i))i∈N.
–The Newton sequence is monotonically increasing, converge s to the least ﬁxed point and does so at least
as fast as the Kleene sequence. More precisely, it satisﬁes
κ(i)⊑ν(i)⊑f(ν(i))⊑ν(i+1)⊑µf= sup
j∈Nκ(j)for all i∈N.
Before giving the formal proof of Theorem 3.9 (see Section 5), we present two ex amples of Newtonian
program analysis , which illustrate the use of our generalized Newton’s method to program analys is.
4 Two case studies
We apply our results to the analysis of two small programs. In the ﬁrst one, a may-alias analysis where
we use the counting semiring, Kleene iteration does not terminate, while Newton’s met hod terminates in
one step. In the second case, an average runtime analysis where we use the real semi ring, neither technique
terminates, but Newton’s method converges substantially faster to the solutio n.
4.1 A May-Alias Analysis
We conduct a may-alias analysis in the spirit of [Deu94]. We consider a program l istify() that transforms a
binary tree (all non-leaf nodes have two children) of pointers into a list of poi nters by reading the nodes of
the tree in preorder. An implementation in C++ could look as follows, where mo veright() follows the right
child pointer, and similarly for move left() and move up().
The ﬂowgraphs of listify(), listifyL(), and listifyR() are shown i n Figure 5.
We wish to compute may-alias information, i.e., information on which data access paths of the tree and
the list may point to the same element. A data access path of the tree can be repres ented as a word over
the alphabet {l,r}: for instance, the path llrcorresponds to the element found as follows: start at the root
node, follow twice the pointer to the left child, then once the pointer to the righ t child, and then the pointer

class Tree {
struct Node {
void ∗data ;
Node ∗left , ∗right ,
∗parent ;
. . .
};
Node ∗root ;
public :
Tree () {. . .}
˜Tree () {. . .}
. . .
void move left () {. . .}
. . .
bool i sl e a f () {. . .}
};
class L i s t i f y {
Tree ∗T;
l i s t<void ∗>L;void l i s t i f y L () {
T. move left ( ) ;
l i s t i f y ( ) ;
T. move up ( ) ;
}
void l i s t i f y R () {
T. move right ( ) ;
l i s t i f y ( ) ;
T. move up ( ) ;
}
void l i s t i f y () {
L. push back (T −>getdata ( ) ) ;
if( T. i s l e a f () == false ){
l i s t i f y L ( ) ; l i s t i f y R ( ) ;
}
}
public :
L i s t i f y () : T(0) , L() {}
void make itso ( Tree& t ) {
T = &t ; T −>gotop ( ) ; l i s t i f y ( ) ;
}
};
Fig. 4. Code snippet of the class Listify which serializes a tree into a l ist.
to the data. Similarly, a data access path of the list can be represented as a wor d over {s}(forsuccessor ).
So may-alias information can be represented as a set of pairs ( w1,w2), where w1∈ {l,r}∗andw2∈ {s}∗.
We are interested in may-alias information at the entry point of listify(), directly before the execution
of L.push back(T →get.data()), which creates an alias. More exactly, we wish to overapprox imate the alias
pairs generated by any valid program path leading from the entry point of li stify to itself, i.e., the “join-
over-all-paths” (or JOP) solution of the program.
Recall from Section 1.1 how we compute the JOP-values of a procedural program: We ﬁrst use Newton’s
method to compute or overapproximate, for any procedure P, the eﬀect of P, denoted by JOP 0(P). Then, the
label of an edge calling Pis replaced by JOP 0(P), and additional edges (labelled with the 1-element) from the
source of the call to the entry point of Pare inserted. The resulting ﬂowgraph no longer contains procedure
calls. For any program point p, we obtain JOP( p) by solving the system of linear dataﬂow equations derived
from that ﬂowgraph. We apply this approach to the listify() program.
In order to guarantee that the computation of JOP 0terminates, we use the Parikh abstraction , in which
we abstract a word w∈ {l,r}∗by a vector (# lw,#rw), where # lwand # rwdenote the number of l’s and
r’s inw. The result of the analysis will be a set of triples ( nl,nr,ns)∈N3. A triple ( nl,nr,ns) indicates
that there may be an alias between some data access path containing nltimes the letter landnrtimes the
letter r, and the (unique) data access path containing nstimes the letter s(thes-th element of the list).
We can then work over the counting semiring described in Section 2.3, with 2N3as carrier. Recall that
the sum operation is set union, and the product operation, denoted by ·c, is given by
N·cM={(nl+ml,nr+mr,ns+ms)|(nl,nr,ns)∈N,(ml,mr,ms)∈M}.
In our abstraction, T.move left() adds 1 to the number of l’s in the data access path of the tree, leaving
the number of r’s and s’s untouched. So we replace the edge label “T.move left()” with the one-element set
{(1,0,0)}. Proceeding similarly with the rest of the edges, we obtain the abstract ﬂowgra phs of Figure 6 (we
omit the curly brackets of one-element sets).

listify
L.push back(T →getdata())
T.isleaf()¬T.isleaf()
listifyL()
listifyR()listifyL
Tmove.left()
listify()
Tmove.up()listifyR
Tmove.right()
listify()
Tmove.up()
Fig. 5. Flowgraph for listify(), listifyL(), and listifyR().
listify
(0,0,1)
(0,0,0)(0,0,0)
listifyL
listifyRlistifyL
(1,0,0)
listify
(-1,0,0)listifyR
(0,1,0)
listify
(0,-1,0)
Fig. 6. Abstract ﬂowgraphs for listify(), listifyL(), and listifyR().
From the abstract ﬂowgraphs we get the equations (with li,li RandliLas abbreviations of listify(),
listifyL() and listifyR()):
li={(0,0,1)} ·c/parenleftbig
{(0,0,0)} ∪ {(0,0,0)} ·cliL·cliR/parenrightbig
liL={(1,0,0)} ·cli·c{(−1,0,0)}
liR={(0,1,0)} ·cli·c{(0,−1,0)}
which can be simpliﬁed applying the commutativity of ·c, yielding liL=liandliR=li. So, we only have to
solve the univariate quadratic equation
li={(0,0,1)} ∪ {(0,0,1)} ·cli·cli. (14)
Kleene iteration does not terminate for (14): we obtain κ(i)={(0,0,2j+ 1)|0≤j≤i}, never reaching
the least solution. But, since our semiring is idempotent and commutative, Theorem 7.7 (see Section 7.1)
guarantees that Newton’s method terminates in one step. It follows that ν(1)={(0,0,2j+ 1)|0≤j}is
the least solution of (14). This is our desired overapproximation of JOP 0. The interpretation is simple: after
termination of listify(), an arbitrary oddnumber of items may have been added to the list, but it is not
possible to have added an even number of items.
As described before, we can use JOP 0to construct a ﬂowgraph without procedure calls, see Figure 7,
where α={(0,0,2j+ 1)|j∈N}and dashed lines indicate edges labelled with (0 ,0,0). Since we are
interested in the value of the JOP for the entry point, we get the linear equatio n
entry ={(0,0,0)} ∪ { (1,0,1)} ·centry ∪ {(0,1,1)} ·cα·centry

listify
entry
(0,0,1)
(0,0,0)(0,0,0)
α
αlistifyL
(1,0,0)
α
(-1,0,0)listifyR
(0,1,0)
α
(0,-1,0)
Fig. 7. Abstract ﬂowgraphs for listify(), listifyL(), and listifyR().
where the second and third terms on the right-hand-side correspond to the loops invol ving listifyL() and
listifyR(). The least solution is
entry =/parenleftbig
{(1,0,1)} ∪ {(0,1,1)} ·cα/parenrightbig∗c
which corresponds to the set
{(nl,nr,ns)∈N3|(nr= 0∧ns=nl)∨(nr>0∧ ∃k∈N:ns= 2nr+nl+ 2k)}.
This result gives the following information on may-aliases:
–A data access path of the tree containing no randnltimes lcan only be aliased to the nl-th element of
the list.
–A data access path of the tree with nr>0 times randnltimes lcan only be aliased to the 2 nr+nl-th
element of the list, or to the larger elements of the same parity.
The problem that Kleene iteration does not terminate for these recursive examples has been addressed
by many researchers. The k-limiting technique was introduced as a way to palliate the problem: basically,
it computes the aliases exactly for data access paths of length at most k, and abstracts the rest very
crudely. The Parikh abstraction can provide information on data access paths of arbitrary depth. It was
used (together with some other features) in [Deu94]. Notice, however, that in our ca se we derive termination
for this abstraction from a generic argument, namely from Theorem 7.7.
4.2 An Average Runtime Analysis
In this example we show how by just changing the semiring our approach can also be applied to average
runtime analysis. We consider a program for lazy evaluation of And/Or-trees. F or this example, an And/Or-
tree is a tree where (i) every node has either zero or two children, (ii) every inner node of t he tree is either
an And-node or an Or-node, and (iii) on any path from the root to a leaf And- and Or-nodes a lternate.
The program constructs and evaluates nodes of the tree (to 0 or 1) only if needed. For inst ance, if the
left subtree of an And-node evaluates to 0, then the program neither constructs nor evaluates the right
subtree. More speciﬁcally, we assume the existence of functions node.leaf(), node.value( ), node.left() and
node.right(), where node.leaf() checks if a node is a leaf, node.value() evaluates a l eaf node, and node.left()
and node.right() create the left and the right child of a node which is not a leaf. Notice that because of lazy
evaluation the program may terminate even if the input is an inﬁnite tree.

function And(node)
ifnode.leaf() then
return node.value()
else
v:= Or(node.left())
ifv= 0then
return 0
else
return Or(node.right())function Or(node)
ifnode.leaf() then
return node.value()
else
v:= And(node.left())
ifv= 1then
return 1
else
return And(node.right())
Figure 8 shows the ﬂowgraph of And(), the one of Or() is similar. We assume t hat the root of the tree is
always an And-node, i.e., the main procedure is And().
And
node.leaf() ¬node.leaf()
return
node.value()v:= Or
v= 0 v/ne}ationslash= 0
return 0
return Or
Fig. 8. Flowgraph for And().
Assume the probabilities that node.leaf() and node.value() return 0 or 1 are known, a s well as the time
taken by each instruction. For our example we assume that all probabilities ar e equal to 0 .5, that node.leaf()
and node.value() take one time unit, and all other instructions take no time. We per form an analysis to
compute (a) the probability that the evaluation terminates (with results 0 or 1 ), and (b) the average runtime.
This corresponds to taking the semiring for the second probabilistic interpretat ion in Section 2.3.
The functions And() and Or() return values, and their control ﬂow depends on the values returned by
calls to node.leaf(), node.value(), and recursive calls to Or() and And(). We need an a nalysis that captures
these dependencies. For this we use a standard instrumentation: we interpret a program procedure, say P,
that may return kdiﬀerent values, say v0,... ,v k−1, askdiﬀerent procedures, P0,... ,P k−1, where Pireturns
vi; more precisely, the control ﬂow of Picontains the valid ﬂow paths of Pthat ﬁnish with return vi. In our
example, we get four procedures: And 0, And 1, Or0and Or 1. The ﬂowgraphs of And 0and And 1are shown in
the ﬁrst row Figure 9. Notice, for instance, that these ﬂowgraphs exclude paths where a ca ll to Or 1(i.e., a
call to Or() that returns 1) is followed by the then branch of “ ifv= 0then return 0”. The ﬂowgraphs of
Or0and Or 1are similar. By construction, the probabilities of termination of And 0() and And 1() are equal
to the probabilities that And() terminates with value 0 and with value 1.
Recall that the semiring values of Section 2.3 are of the form ( p,d), where p∈[0,1] stands for the
probability of a given set of paths and d∈[0,∞) for their expected execution time (duration). The second row
of Figure 9 shows how to assign semiring values to the edges. For instance, the edge labelled by node.leaf 1()
gets (0 .5,1) as semiring value, because node.leaf() returns 1 with probability 0 .5, and it takes one unit of
time.

And0
node.leaf 1() node.leaf 0()
return
node.value 0()v:= Or 0 v:= Or 1
v= 0 v/ne}ationslash= 0
return 0
return Or 0And1
node.leaf 1() node.leaf 0()
return
node.value 1()v:= Or 1
v/ne}ationslash= 0
return Or 1
And0
(0.5,1) (0.5,1)
(0.5,1)Or0 Or1
(1,0) (1,0)
(1,0)
Or0And1
(0.5,1) (0.5,1)
(0.5,1)Or1
(1,0)
Or1
Fig. 9. Flowgraphs for the procedures And 0and And 1.
Using the framework of Section 1.1, the probability that a procedure returns a value and the expected
time to return this value is given as the least solution of the following eq uation system:
And0= (0.25,2) + e(0.5,1)·e(Or0+eOr1·eOr0)
And1= (0.25,2) + e(0.5,1)·eOr1·eOr1
Or0= (0.25,2) + e(0.5,1)·eAnd0·eAnd0
Or1= (0.25,2) + e(0.5,1)·e(And 1+eAnd0·eAnd1)(15)
where + eand·eare the semiring operations deﬁned in Section 2.3.
The equation system (15) happens to be solvable analytically. For instance, t he And 0-component of the
least solution is/parenleftBig√
10
2−1,19
6+37√
10
30/parenrightBig
≈(0.581,7.067). This means that the procedure And() terminates and
returns the value 1 with probability 0 .581 and needs in average 7 .067 time steps to do so. For equation systems
stemming from larger programs, the solution may not be representable by ro ots, cf. [EY09]. Therefore,
approximation methods are generally needed. We compute the ﬁrst elements of the Kleene and New ton
sequences for (15). Rounding to three decimals we obtain:
i κ(i)
And0ν(i)
And0κ(i)
And1ν(i)
And1
0 (0.250,2.000) (0 .250,2.000) (0 .250,2.000) (0 .250,2.000)
1 (0.406,2.538) (0 .495,3.588) (0 .281,2.333) (0 .342,3.383)
2 (0.448,2.913) (0 .568,5.784) (0 .333,3.012) (0 .409,5.906)
3 (0.491,3.429) (0 .581,6.975) (0 .350,3.381) (0 .419,7.194)
4 (0.511,3.793) (0 .581,7.067) (0 .370,3.904) (0 .419,7.295)

We have κ(i)
Or0=κ(i)
And1andν(i)
Or0=ν(i)
And1and similarly for Or 1. We observe that the Newton sequence
converges faster than the Kleene sequence. In particular, while the ﬁrst entry of ν(4)
And0is>0.58, further
computation shows that i= 21 is the smallest index isuch that the ﬁrst entry of κ(i)
And0is>0.58.
The performance gap between Kleene and Newton iteration can be widened by lowering the leaf proba-
bility from 0.5 to 0.4. In this case, the procedure And() takes, in average, a ti me of about 29 .81 to return the
value 0; in other words, in this case, the second entry of the And 0-component of the least solution of (15) is
approximately 29.81. It takes around 222 Kleene iterations to determine that thi s value is greater than 29.8,
whereas 6 Newton iterations suﬃce to establish the same fact. Actually, numeri cal analysis shows that when
the leaf probability tends to (√
33−5)/2≈0.372, the average runtime tends to inﬁnity, and the gap between
Newton and Kleene iteration grows unboundedly. However, it should be mentioned that a Newton step is
more expensive in general than a Kleene step, since a Newton step requires solving a li near equation system
of dimension 4. In [KLE07,EKL08] and [EKLBP] we have given a detailed analys is of the convergence speed
of Newton’s method applied to (numerical) ﬁxed-point equations. In general, the mor e precision is required,
the better is the performance of Newton’s method compared to Kleene iteration.
5 Proof of Fundamental Properties of the Newton Sequences
In this section we prove Theorem 3.9 which states that there exists exactly one N ewton sequence, that it
converges to the least ﬁxed point, and that it does so at least as fast as the Kleene sequence. The proof is
split in two propositions. Proposition 5.6 in Section 5.1 states that there is only one Newton sequence. The
following proposition covers the rest of Theorem 3.9:
Proposition 5.1. Letf:V→Vbe a vector of power series.
–For every Newton approximant ν(i)there exists a vector δ(i)such that f(ν(i)) =ν(i)+δ(i). So there is
at least one Newton sequence.
–Any Newton sequence satisﬁes κ(i)⊑ν(i)⊑f(ν(i))⊑ν(i+1)⊑µf= supj∈Nκ(j)for all i∈N.
The proof of Proposition 5.1 is based on two lemmata. The ﬁrst one, an easy consequence of Kleene’s
theorem, provides a closed form for the least solution of a linear system of ﬁxed-p oint equations in terms of
the Kleene star operator, deﬁned as follows:
Deﬁnition 5.2. Letg:V→Vbe a monotone map. The map g∗:V→Vis deﬁned as g∗(v) :=/summationtext
i∈Ngi(v),
where g0(v) :=v,gi+1(v) :=g(gi(v))for every i≥0. Similarly, we set for all j∈N:g≤j:=/summationtext
0≤i≤jgi(v).
The existence of/summationtext
i∈Ngi(v) is guaranteed by the properties of ω-continuous semirings. Observe that
v⊑g∗(v) and g∗(v) =v+g(g∗(v)) hold.
Lemma 5.3. Letf:V→Vbe a vector of power series, and u,v∈V. Then the least solution of D f|u(X)+
v=Xis Df|∗
u(v). In particular, a Newton sequence from Deﬁnition 3.6 can be e quivalently deﬁned by setting
ν(0)=f(0)andν(i+1)=ν(i)+Df|∗
ν(i)(δ(i)).
Proof. Setg(X) :=Df|u(X) +v. The vector gis a power series in every component and thus a monotone
map from VtoV. By Kleene’s ﬁxed-point theorem, the least solution of g(X) =Xis given by sup {gi(0)|
i∈N}= sup {Df|≤i
u(v)|i∈N}=Df|∗
u(v). ⊓ ⊔
The second lemma, which is interesting by itself, is a generalization of Tayl or’s theorem to arbitrary
ω-continuous semirings.
Lemma 5.4. Letf:V→Vbe a vector of power series and let u,vbe two vectors. We have
f(u) +Df|u(v)⊑f(u+v)⊑f(u) +Df|u+v(v).
Proof. It suﬃces to show those inequalities for each component separately, so let w.l.o .g.f=f:V→Sbe
a power series. We proceed by induction on the construction of f. The base case (where fis a constant)

and the case where fis a sum of polynomials are easy, and so it suﬃces to consider the case in which fis a
monomial. So let
f=g·X·a
for a monomial g, a variable X∈ Xand a constant a. We have
f(u) =g(u)·uX·aand Df|u(v) =g(u)·vX·a+Dg|u(v)·uX·a .
By induction we obtain:
f(u+v) =g(u+v)·(uX+vX)·a
⊒/parenleftbig
g(u) +Dg|u(v)/parenrightbig
·(uX+vX)·a
=g(u)·uX·a+g(u)·vX·a+Dg|u(v)·(uX+vX)·a
⊒f(u) +g(u)·vX·a+Dg|u(v)·uX·a
=f(u) +Df|u(v)
and
f(u+v) =g(u+v)·(uX+vX)·a
⊑/parenleftbig
g(u) +Dg|u+v(v)/parenrightbig
·(uX+vX)·a
=g(u)·uX·a+g(u)·vX·a+Dg|u+v(v)·(uX+vX)·a
⊑f(u) +g(u+v)·vX·a+Dg|u+v(v)·(uX+vX)·a
=f(u) +Df|u+v(v) ⊓ ⊔
We can now proceed to prove Proposition 5.1.
Proof (of Proposition 5.1). First we prove for all i∈Nthat a suitable δ(i)exists and, at the same time, that
the inequality κ(i)⊑ν(i)⊑f(ν(i)) holds. We proceed by induction on i. The base case i= 0 is easy. For
the induction step, let i≥0.
κ(i+1)=f(κ(i)) (deﬁnition of κ(i))
⊑f(ν(i)) (induction: κ(i)⊑ν(i))
=ν(i)+δ(i)for some δ(i)(induction)
⊑ν(i)+Df|∗
ν(i)(δ(i)) ( v⊑g∗(v))
=ν(i+1)(Lemma 5.3)
=ν(i)+δ(i)+Df|ν(i)(Df|∗
ν(i)(δ(i))) ( g∗(v) =v+g(g∗(v)) )
=f(ν(i)) +Df|ν(i)(Df|∗
ν(i)(δ(i))) (deﬁnition of δ(i))
⊑f(ν(i)+Df|∗
ν(i)(δ(i))) (Lemma 5.4)
=f(ν(i+1)) (Lemma 5.3)
Since ν(i+1)⊑f(ν(i+1)), there exists a δ(i+1)such that ν(i+1)+δ(i+1)⊑f(ν(i+1)). Next we prove f(ν(i))⊑
ν(i+1):
f(ν(i)) =ν(i)+δ(i)(as proved above)
⊑ν(i)+Df|∗
ν(i)(δ(i)) ( v⊑g∗(v))
=ν(i+1)(Lemma 5.3)
It remains to prove supj∈Nκ(j)=µfandν(i)⊑µffor all i. The equation supj∈Nκ(j)=µfholds by
Kleene’s theorem (Proposition 2.4). To prove ν(i)⊑µffor all iwe need a lemma.

Lemma 5.5. Letf(x)⊒x. For all d≥0there exists a vector e(d)(x)such that
fd(x) +e(d)(x) =fd+1(x)and
e(d)(x)⊒Df|fd−1(x)(Df|fd−2(x)(...Df|x(e(0)(x))...))
⊒Df|d
x(e(0)(x)).
Proof of the lemma. By induction on d. Ford= 0 there is an appropriate e(0)(x) by assumption. Let
d≥0.
fd+2(x) =f(fd(x) +e(d)(x)) (induction)
⊒fd+1(x) +Df|fd(x)(e(d)(x)) (Lemma 5.4)
⊒fd+1(x) +Df|fd(x)(...Df|x(e(0)(x))...) (induction)
Therefore, there exists an e(d+1)(x)⊒Df|fd(x)(...Df|x(e(0)(x))...).Since Df|yis monotone in yand
x⊑f(x)⊑f2(x)⊑..., the second inequality also holds. This completes the proof of the lemma. ⊓ ⊔
Notice that Lemma 5.5 holds for x=ν(i)ande(0)(ν(i)) =δ(i), because we have already shown ν(i)⊑
f(ν(i)). Now we can prove ν(i)⊑µfby induction on i. The case i= 0 is trivial. Let i≥0. We have:
ν(i+1)=ν(i)+Df|∗
ν(i)(δ(i)) (Lemma 5.3)
=ν(i)+/summationdisplay
d∈NDf|d
ν(i)(δ(i)) (deﬁnition of Df|∗
ν(i))
⊑ν(i)+/summationdisplay
d∈Ne(d)(ν(i)) (Lemma 5.5)
= sup
d∈Nfd(ν(i)) ( ω-continuity)
⊑µf (induction:
ν(i)⊑f(ν(i))⊑f(f(ν(i)))⊑...⊑µf)
This completes the proof of Proposition 5.1. ⊓ ⊔
5.1 Uniqueness
In Deﬁnition 3.6 the Newton approximant ν(i)is deﬁned in terms of a vector δ(i)satisfying ν(i)+δ(i)=
f(ν(i)). In the previous section we have shown that such a vector always exists. How ever, in a semiring there
there may be multiple such δ(i)’s, and so in principle there could be multiple Newton sequences. We show
now that this is notthe case, i.e., there is only one Newton sequence ( ν(i))i∈N, independent of the choice of
δ(i):
Proposition 5.6. Letf:V→Vbe a vector of power series. There is exactly one Newton seque nce
(ν(i))i∈N.
Theorem 3.9 follows directly by combining Proposition 5.1 and Propositi on 5.6. So for Theorem 3.9 it remains
to prove Proposition 5.6, which we do in the rest of this section.
It is convenient for this proof to introduce substitutionals , a notion related to diﬀerentials, see Deﬁni-
tion 3.5.
Deﬁnition 5.7. Letfbe a power series over an ω-continuous semiring Sand let s∈N+. The substitutional
offw.r.t. sat the point vis the mapping $sf|v:V→Sdeﬁned as follows:
Iffis a monomial, i.e., of the form f=a1X1· · ·akXkak+1, then
$sf|v(b) =/braceleftBigg
a1vX1· · ·as−1vXs−1asbXsas+1vXs+1· · ·akvXkak+1if1≤s≤k
0 otherwise.

Iffis a power series, i.e., of the form f=/summationtext
i∈Ifi, then
$sf|v(b) =/summationdisplay
i∈I$sfi|v(b).
In words: if fis a monomial with at least svariables then $sf|v(b)is obtained from fby replacing the s-th
variable XsbybXsand all other variables by the corresponding component of v. Iffis a monomial with
less than svariables then $sf|v(b) = 0. Iffis a power series then the substitutional of fis the sum of the
substitutionals of f’s monomials.
Analogously to diﬀerentials, we extend the deﬁnition of sub stitutionals to vectors of power series by
applying the substitution componentwise. Formally, we deﬁ ne the substitutional of a vector of power series f
atvas the function $sf|v:V→Vwith
($sf|v(b))X:= $ sfX|v(b).
Observe that, like the diﬀerential (see Remark 3.7), the substitutional is “l inear”, i.e., $ sf|v(b+b′) =
$sf|v(b) + $ sf|v(b′).
Notation 1. For any j∈Nand any sequence s= (s1,... s j)∈Nj
+we write $sf|v(b)for
$s1f|v($s2f|v(· · ·$sjf|v(b)· · ·)), and $sf|v(b) =bifj= 0.
The following facts are immediate from the deﬁnitions.
Proposition 5.8. Letfbe a monomial. Then
DXf|v(b) =/summationdisplay
{$sf|v(b)|Xis the s-th variable in f}.
Letfbe a vector of power series. Then:
1. Df|v(b) =/summationtext
s∈N+$sf|v(b).
2. Df|j
v(b) =/summationtext
s∈Nj
+$sf|v(b).
3. For all s∈N+we have f(v)⊒$sf|v(v).
Example 5.9. Consider the polynomial f=aXY X +cY. Then
$1f|v(b) =abXvYvX+cbY
$2f|v(b) =avXbYvX
$3f|v(b) =avXvYbX
DXf|v(b) =abXvYvX+avXvYbX
DYf|v(b) =avXbYvX+cbY.
Observe that Df|v(b) =DXf|v(b)+DYf|v(b) = $ 1f|v(b)+$2f|v(b)+$3f|v(b) and that f(v) =avXvYvX+
cvY⊒$sf|v(v) holds for all s∈N+. ⊓ ⊔
For the proof of Proposition 5.6 we need the following two lemmata.
Lemma 5.10. Letfbe a vector of power series. Let ν+δ=f(ν). Letj∈Nand(s1,... ,s j+1)∈Nj+1
+.
Then ν+Df|≤j
ν(δ)⊒$(s1,...,s j+1)f|ν(ν).
Proof. By induction on j. Forj= 0 we have ν+Df|≤0
ν(δ) =ν+δ=f(ν)⊒$s1f|ν(ν) by Proposition 5.8.3.
Letj≥0. We have:
ν+Df|≤j+1
ν(δ) =ν+Df|≤j
ν(δ) +Df|j+1
ν(δ)
⊒$(s1,...,s j+1)f|ν(ν) +Df|j+1
ν(δ) (induction)
⊒$(s1,...,s j+1)f|ν(ν) + $ (s1,...,s j+1)f|ν(δ) (Prop. 5.8.2.)
= $(s1,...,s j+1)f|ν(f(ν)) ( ν+δ=f(ν))
⊒$(s1,...,s j+1)f|ν($sj+2f|ν(ν)) (Prop. 5.8.3.)
= $(s1,...,s j+2)f|ν(ν) ⊓ ⊔

Lemma 5.11. Letfbe a vector of power series. Let ν+δ=ν+δ′=f(ν). Then ν+Df|∗
ν(δ) =
ν+Df|∗
ν(δ′).
Proof. We show ν+Df|≤j
ν(δ) =ν+Df|≤j
ν(δ′) for all j∈N. Then the lemma follows by ω-continuity. We
proceed by induction on j. The induction base ( j= 0) is clear. Let j≥0. We have:
ν+Df|≤j+1
ν(δ) =ν+Df|≤j
ν(δ) +Df|j+1
ν(δ)
=ν+Df|≤j
ν(δ′) +Df|j+1
ν(δ) (induction)
=ν+Df|≤j
ν(δ′)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=:u+/summationdisplay
s∈Nj+1
+$sf|ν(δ) (Prop. 5.8.2.)
By Lemma 5.10, we have u⊒$sf|ν(ν) for all s∈Nj+1
+. In other words, for all s∈Nj+1
+there is a u′
such that u=u′+ $sf|ν(ν). Hence, for all s∈Nj+1
+, we have u+ $sf|ν(δ) =u′+ $sf|ν(ν) + $ sf|ν(δ) =
u′+ $sf|ν(f(ν)) =u+ $sf|ν(δ′). Therefore, in the above equation, we can replace δbyδ′due to the
“presence” of u:
=ν+Df|≤j
ν(δ′) +/summationdisplay
s∈Nj+1
+$sf|ν(δ′) (as argued above)
=ν+Df|≤j
ν(δ′) +Df|j+1
ν(δ′) (Prop. 5.8.2.)
=ν+Df|≤j+1
ν(δ′) ⊓ ⊔
Now Proposition 5.6 follows immediately from Lemma 5.11 by a stra ightforward inductive proof. ⊓ ⊔
6 Derivation Trees and the Newton Approximants
The proofs of the previous section were purely algebraical. For deeper and stronger res ults we need the
notion of derivation trees . To this end we reinterpret a system of power-series as a context-free grammar,
and assign it a set of derivation trees . We then characterize the Kleene and Newton approximants of the
system in terms of those trees. This characterization of the Newton approxim ants will be crucially used in
the rest of this paper.
We assume that the reader is familiar with the notion of derivation tree of a context-free grammar. Recall
that the yield of a derivation tree (obtained by reading the leaves from left to r ight) is a word generated by
the grammar, and every word generated by the grammar is the yield of one or mo re derivation trees. In our
reinterpretation the non-terminals will be the variables of the system of pow er series, and the terminals will
be its coeﬃcients.
We show that the Kleene approximants κ(i)are equal to the sum of the yields of the derivation trees
having a certain height. Similarly, we show that the Newton approximants ν(i)are equal to the sum of the
yields of the trees having a certain dimension , a notion introduced in Deﬁnition 6.7 below.
For the rest of the section we ﬁx a vector fof power series over a ﬁxed but arbitrary ω-continuous
semiring. Without loss of generality, we assume that fX=/summationtext
j∈JmX,jholds for every variable X∈ X, i.e.,
we assume that for all variables the sum is over the same countable set Jof indices.
Consider the set of ordered trees whose nodes are labelled by pairs ( X,j), where X∈ Xandj∈J.
Sometimes we identify a tree and its root. In particular, we say that a tree tis labelled by ( X,j) if its root
is labelled by ( X,j). The mappings λ,λvandλmare deﬁned by λ(t) := (X,j),λv(t) :=X, and λm(t) :=j.
Given a set Tof trees, we denote by TXthe set of trees t∈Tsuch that λv(t) =X.
We deﬁne the set of derivation trees of f, and show how to assign to each tree a semiring element called the
yield of the tree. For technical reasons our deﬁnition diﬀers slightly from the st raightforward generalization
of derivation trees for grammars.
Deﬁnition 6.1 ((derivation tree, yield)). Thederivation trees offand their yields are inductively
deﬁned as follows:

–For every monomial mX,joffX, if no variable occurs in mX,j, then the tree tconsisting of one single
node labelled by (X,j)is a derivation tree of f. Its yield Y(t)is equal to mX,j.
–LetmX,j=a1X1a2X2... a kXkak+1for some k≥1, and let t1,... ,t kbe derivation trees of fsuch that
λv(ti) =Xifor1≤i≤k. Then the tree tlabelled by (X,j)and having t1,... ,t kas (ordered) children
is also a derivation tree of f, and its yield Y(t)is equal to a1Y(t1)... a kY(tk)ak+1.
TheyieldY(T)of a countable set Tof derivation trees is deﬁned by Y(T) =/summationtext
t∈TY(t). In the following,
we mean derivation tree whenever we say tree.
Example 6.2. Figure 10 shows a system of equations (system (1) from the introduction, on the left). The
basic idea is to read these equations as rules of a context-free grammar, e.g., t he equation X=aXY +b
is interpreted as the rules X→aXY andX→b. By this reinterpretation derivation trees are naturally
associated with the given equation system. But as addition is not assumed to be idempotent in general,
we have to extend the standard deﬁnition of derivation tree in order to handle multipl icities correctly. The
derivation tree depicted in the middle of Figure 10 therefore records which monomia l of which variable gives
rise to the children of a given node. For instance, consider the node labelled by ( Y,1) (the right child of
the root). Since the ﬁrst monomial of the equation for YiscY Z, the node has two children, say c1,c2with
λv(c1) =Yandλv(c2) =Z. Asλm(c2) = 2, the children of c2are determined by the second monomial of
the equation for Z. Since this monomial is h, which contains no variables, c2has no children. The right part
of the ﬁgure shows the result of labelling each node of the tree with the yield of t he subtree rooted at it.
X=aXY+b
Y=cY Z+dY X +e
Z=gXh+i(X,1)
(X,2) ( Y,1)
(Y,2) ( Z,2)
(Y,3) (X,2)abcdebi
b cdebi
deb i
e b
Fig. 10. A system of equations, a derivation tree, and its yield
6.1 Kleene Sequence and Height
As a warm-up for the Newton case, we characterize the Kleene sequence ( κ(i))i∈Nin terms of the derivation
trees of a certain height.
Deﬁnition 6.3 ((height)). Lettbe a derivation tree. The height oft, denoted by h(t), is the length (number
of edges) of a longest path from the root to some leaf. We denot e byHithe set of derivation trees of height
at most i.
Proposition 6.4./parenleftbig
κ(i)/parenrightbig
X=Y(Hi
X), i.e., the X-component of the i-th Kleene approximant κ(i)is equal to
the yield of Hi
X.
The proof can be found in Appendix A.
Notice that Proposition 6.4 no longer holds if nodes are only labelled with a variable, and not with
a pair. Consider for instance the equation X=a+a, for which κ(0)=a+a. There are two derivation
trees t1,t2of height 0, both consisting of one single node: t1is labelled by ( X,1), and t2by (X,2). We
getY(t1) +Y(t2) =a+a=κ(0). If we labelled nodes only with variables, then there would be one single
derivation tree t, and we would get Y(t) =a, which in general is diﬀerent from a+a.

Example 6.5. Consider again the equation X= 1/2·X2+ 1/2 over the real semiring. We have κ(2)=
89/128. Figure 11 shows the ﬁve derivation trees of height at most 2. It is easy to see that their yields are
1/2,1/8,1/32,1/32,1/128, which add up to 89 /128.
(X,2)(X,1)
(X,2) (X,2)
(X,1)
(X,2) ( X,1)
(X,2) (X,2)(X,1)
(X,1) ( X,2)
(X,2) (X,2)(X,1)
(X,1) ( X,1)
(X,2) (X,2) (X,2) (X,2)
Fig. 11. Trees of height at most 2 for the equation X= 1/2·X2+ 1/2.
By Kleene’s theorem we obtain that the least solution of the equation system is equa l to the yield of the
set of all trees.
Corollary 6.6. LetTbe the set of all derivation trees of f. For all X∈ X:(µf)X=Y(TX).
Proof. By Kleene’s Theorem (Proposition 2.4) we have ( µf)X= supi∈N(κ(i))X. The result follows from
Proposition 6.4. ⊓ ⊔
6.2 Newton Sequence and Dimension
We introduce a second parameter of a tree, namely its dimension . Like the height, it depends only on the
tree structure, and not on the labels of its nodes. Loosely speaking, a tree has dim ension 0 if it consists
of just one node; a tree has dimension iif there is a path from its root to some node which has at least 2
children with dimension i−1 and all subtrees of the path that are not themselves on the path have dimension
at most i−1. The path is called the backbone of the tree. The geometric intuition for the name dimension
is that a tree of dimension ican be naturally represented in Ri: a tree of dimension 1 can essentially be
represented as a line (with small “spikes”, see Figure 12(b)); a tree of dimens ion 2 can be drawn in the plane,
with the backbone as a line, and the subtrees of dimension 1 as lines perpendicular to the backbone (see
Figure 12(c)). In general, the subtrees of an i-dimensional tree are drawn in hyperplanes orthogonal to the
line for the backbone, yielding a representation in Ri. To the best of our knowledge, the notion of dimension
has not been used before. Formally, we use an inductive deﬁnition of dimension that is more convenient for
proofs.
Deﬁnition 6.7 ((dimension)). Thedimension d(t)of a tree tis inductively deﬁned as follows:
1. Ifthas no children, then d(t) = 0.
2. Ifthas exactly one child t1, then d(t) =d(t1).
3. Ifthas at least two children, let t1,t2be two distinct children of tsuch that d(t1)≥d(t2)andd(t2)≥d(t′)
for every child t′/\e}a⊔io\slash=t1. Letd1=d(t1)andd2=d(t2). Then
d(t) =/braceleftbiggd1+ 1 ifd1=d2
d1 ifd1> d2.

t<i
t<i
t<i
ti−1 ti−1
(a) (b) (c)
Fig. 12. (a) shows the general structure of a tree of dimension i, where t<i(resp. ti−1) represents any tree of dimension
< i(resp. = i−1). (b) and (c) give some idea of the topology of one-, resp. two-dimensional t rees.
We denote by Dithe set of derivation trees of dimension at most i.
Remark: It is easy to prove by induction that h(t)≥d(t) holds for every derivation tree t.
In the rest of the section we show that the i-th Newton approximant ν(i)is equal to the yield of the
derivation trees of dimension at most i:
Theorem 6.8 (Tree Characterization of the Newton Sequence). Let(ν(i))i∈Nbe the Newton sequence
off. For every X∈ Xand every i≥0we have/parenleftbig
ν(i)/parenrightbig
X=Y(Di
X), i.e., the X-component of the i-th Newton
approximant is equal to the yield of Di
X.
The proof is as follows. We deﬁne, in terms of trees, a sequence ( τ(i))i∈Nsatisfying τ(i)
X=Y(Di
X)
(Lemma 6.10), and we prove that it is a Newton sequence (Lemma 6.11). As the N ewton sequence is unique
by Proposition 5.6, we have τ(i)=ν(i)and Theorem 6.8 follows.
We need the following deﬁnition.
Deﬁnition 6.9. A tree tisproper ifd(t)> d(t′)for every child t′oft. For every i≥0, letPibe the set of
proper trees of dimension i. Deﬁne the sequence (τ(i))i∈Nas follows:
τ(0)=f(0)
τ(i+1)=τ(i)+Df|∗
τ(i)(δ(i)),
where δ(i)
X=Y(Pi+1
X)for all X∈ X.
Lemma 6.10. For every variable X∈ Xand every i≥0:τ(i)
X=Y(Di
X).
Lemma 6.11. The sequence (τ(i))i∈Nis a Newton sequence as deﬁned in Deﬁnition 3.6, i.e., the δ(i)of
Deﬁnition 6.9 satisfy f(τ(i)) =τ(i)+δ(i).
The proofs of Lemma 6.10 and Lemma 6.11 can be found in Appendix A.
Example 6.12. Let us recall our example from the introduction (cf. Fig. 1) with the equations
X=a·X·Y+b
Y=c·Y·Z+d·Y·X+e
Z=g·X·h+i.
Using our characterizations of κ(i)andν(i)by means of derivation trees we see that (a) every derivation tree
trepresents a terminating run of the procedure λ(t), and, thus, (b) while κ(i)only corresponds to a ﬁnite set
of trees (runs), for i >0 every ν(i)corresponds to an inﬁnite set of runs. Hence, it is not very surprising that
in general the Newton approximants give a better approximation of the (abst ract) semantics of a program
than the Kleene approximants. ⊓ ⊔

7 Idempotent Semirings
Recall that in the algebraic structure underlying the framework of [SP81] the summ ation operator is given by
the join of a semilattice and, thus, summation is idempotent. We therefore s tudy in this section the properties
of our generalized Newton’s method for this special case of ω-continuous semirings satisfying the additional
axiom of idempotent addition. We simply call such semirings idempotent ω-continuous semirings, or just
idempotent semirings in the following. In idempotent semirings, the natural order can be characterized as
follows: a⊑bholds if and only if a+b=b. This is because a⊑bmeans by deﬁnition that there is a csuch
thata+c=b. Then we have a+b=a+a+c=a+c=b. This extends analogously to vectors.
We start by showing that in the idempotent case the deﬁnition of the Newton sequence ( ν(i))i∈Ncan be
simpliﬁed.
Proposition 7.1. Letfbe a vector of power series over an idempotent semiring. Let (ν(i))i∈Ndenote the
Newton sequence of f. It satisﬁes the following equations for all i∈N:
(a)ν(i+1)=Df|∗
ν(i)(f(ν(i)))
(b)ν(i+1)=Df|∗
ν(i)(ν(i))
(c)ν(i+1)=Df|∗
ν(i)(f(0))
Proof. We ﬁrst show (a). By Theorem 3.9 we have ν(i)⊑f(ν(i)), hence with idempotence ν(i)+f(ν(i)) =
f(ν(i)). So we can choose δ(i)=f(ν(i)) and have ν(i+1)=ν(i)+Df|∗
ν(i)(f(ν(i))) =Df|∗
ν(i)(f(ν(i))),
because ν(i)⊑f(ν(i))⊑Df|∗
ν(i)(f(ν(i))). So (a) is shown.
Again by Theorem 3.9 we have f(0) = ν(0)⊑ν(i)⊑f(ν(i)). So we have Df|∗
ν(i)(f(0))⊑
Df|∗
ν(i)(ν(i))⊑Df|∗
ν(i)(f(ν(i))). Hence, for (b) and (c), it remains to show Df|∗
ν(i)(f(ν(i)))⊑Df|∗
ν(i)(ν(i))
andDf|∗
ν(i)(ν(i))⊑Df|∗
ν(i)(f(0)), respectively. For (b) we have:
Df|∗
ν(i)(f(ν(i)))
⊑Df|∗
ν(i)(f(0) +Df|ν(i)(ν(i))) (Lemma 5.4)
=Df|∗
ν(i)(f(0)) +Df|∗
ν(i)(Df|ν(i)(ν(i)))
⊑Df|∗
ν(i)(ν(i)) +Df|∗
ν(i)(Df|ν(i)(ν(i))) ( f(0)⊑ν(i))
⊑Df|∗
ν(i)(ν(i)) +Df|∗
ν(i)(ν(i)) (Lemma 5.3)
=Df|∗
ν(i)(ν(i)) (idempotence)
So (b) is shown.
For (c) it remains to show Df|∗
ν(i)(ν(i))⊑Df|∗
ν(i)(f(0)). We proceed by induction on i. The base case
i= 0 is easy because ν(0)=f(0). Let i≥1. We have:
Df|∗
ν(i)(ν(i))
=Df|∗
ν(i)(Df|∗
ν(i−1)(ν(i−1))) (by (b))
⊑Df|∗
ν(i)(Df|∗
ν(i−1)(f(0))) (by induction)
⊑Df|∗
ν(i)(Df|∗
ν(i)(f(0))) (Theorem 3.9: ν(i−1)⊑ν(i))
=Df|∗
ν(i)(f(0)) (see explanation below)
For the last step we used that in the idempotent case we have g∗(g∗(x)) =g∗(x) for any linear map g:
V→V. Recall that Remark 3.7 states that Df|ν(i)is linear.
g∗(g∗(x)) =/summationdisplay
j∈Ngj/parenleftBigg/summationdisplay
k∈Ngk(x)/parenrightBigg
(Deﬁnition 5.2)
=/summationdisplay
j∈N/summationdisplay
k∈Ngj(gk(x)) (linearity)

=/summationdisplay
l∈Ngl(x) (idempotence)
=g∗(x) (Deﬁnition 5.2)
This concludes the proof. ⊓ ⊔
In the rest of the section we study commutative idempotent semirings. where not only addition is idem-
potent, but multiplication is commutative. We will use the abbreviation ci-semirings for such ω-continuous
semirings in the following.
An instance of the Newton sequence in a ci-semiring has already been presented in the counti ng semiring
example on page 11. We show another one here.
Example 7.2. Let/a\}bracke⊔le{⊔2{a}∗,+,·,0,1/a\}bracke⊔ri}h⊔denote the ci-semiring /a\}bracke⊔le{⊔2{a}∗,∪,·,∅,{ε}/a\}bracke⊔ri}h⊔. The multiplication ·is meant to
be commutative. For simplicity, we write aiinstead of {ai}. Consider f(X1,X2) = (X2
2+a, X2
1). We have:
Df|(v1,v2)(X1,X2) =/parenleftbig
v2X2, v1X1/parenrightbig
and
Df|∗
(v1,v2)(X1,X2) = (v1v2)∗/parenleftbig
X1+v2X2, v1X1+X2/parenrightbig
.
The ﬁrst three elements of the Newton sequence are:
ν(0)= (a,0),ν(1)= (a,a2),ν(2)= (a3)∗(a,a2).
It is easy to check that ν(2)is a ﬁxed point of f. Hence we have ν(2)=µf, asν(2)⊑µfby Theorem 3.9. ⊓ ⊔
In the case of ci-semirings the behaviours of the Kleene and Newton sequence diﬀer very much: while
the Kleene sequence may still need inﬁnitely many steps, the Newton sequence always reaches µfafter
ﬁnitely many. This was ﬁrst shown by Hopkins and Kozen in 7.3. Hopkins and Kozen deﬁned the sequence
(ν(i))i∈Ndirectly through the equations ν(0)=f(0) and ν(i+1)=Df|∗
ν(i)(ν(i)) from Proposition 7.1 (b),
without noticing the connection to Newton’s method (which is not surprising, since in the idempotent case
the original equations get masked). They proved the following result, which giv es aO(3n) upper bound for
the number of Newton iterations required for a system of nequations:
Theorem 7.3 ([HK99]). Letfbe a vector of power series over a ci-semiring and a set Xof variables with
|X|=n. There is a function P:N→NwithP(n)∈ O(3n)such that ν(P(n))=µf.
In Section 7.1 we improve Theorem 7.3 by showing that it holds with P(n) =n. This is achieved through
our characterisation of the Newton approximants in terms of derivation t rees.
7.1 Analysis of the Convergence Speed
We analyze how many steps the Newton iteration and, equivalently, the Hopkins-Ko zen iteration need to
reach µfwhen we consider ci-semirings.
Recall from Section 6 the concept of derivation trees (short: trees). A tree thas a height h(t), a dimension
d(t), and a yield Y(t). We deﬁne yet another tree property.
Deﬁnition 7.4. A tree tiscompact ifd(t)≤L(t), where L(t)denotes the number of distinct λv-labels in t.
Now we are ready to prove the key lemma of this section, which states that any t ree can be made compact.
Lemma 7.5. For each tree tthere is a compact tree t′withλv(t) =λv(t′)andY(t) =Y(t′).
Example 7.6. We ﬁrst sketch the proof of the lemma by means of an example. Consider the follow ing uni-
variate polynomial equation system:
X=f(X) :=X2+a+b.
Consider now the following tree t∈ TX.6
6To improve readability in the following illustrations, we replace the node labels ( X,1), (X,2), (X,3) by ( X, X2),
(X, a), (X, b), respectively.

(X,X2)
(X,X2) (X,X2)
(X,a)(X,b)(X,a)(X,a)
This tree has dimension 2 and is therefore not compact by deﬁnition. In order to make it compact, we
have to transform it into a derivation tree of fwhich is of dimension 1 without changing its yield nor the
variable-label of the root.
The idea is to reduce the left subtree to a tree of dimension 0 by reallocating “pump trees ” (encircled
in the above ﬁgure) into the right subtree; after that, we deal recursively with t he right subtree.7We ﬁrst
remove such a pump tree from the rest of the tree by deleting the connecting edges and connecting t he
remaining parts as depicted here:
(X,X2)
(X,X2) (X,X2)
(X,a)(X,b)(X,a)(X,a)
Note that we can introduce the new edge because the roots of the pump tree and the remaini ng subtree, in
our example the left-most leaf, are labeled by the same variable. Next, we rea llocate the detached pump tree
into the right subtree, e.g. as shown here:
(X,X2)
(X,X2) (X,X2)
(X,a)(X,b)(X,a)(X,a)
It is easy to check that this new tree is indeed a derivation tree of f, and has the same yield as the original
one. Further this tree is already compact. In general, we would have to proceed recursivel y in order to make
the right subtree compact.
Note that, as we assume multiplication to be commutative, it is not impo rtant where we insert the pump
tree into the right subtree. In the following proof we show that we can alway s ﬁnd such pump trees and
relocate them, i.e. ﬁnd insertion points, if the tree under consideration is not compa ct. ⊓ ⊔
We now give a formal proof of Lemma 7.5:
Proof. We write t=t1·t2to denote that tis combined from t1andt2in the following way: The tree t1is a
“partial” derivation tree, i.e., a regular derivation tree except for one leaf lmissing its children. The tree t2
is a derivation tree with λv(t2) =λv(l). The tree tis obtained from t1andt2by replacing the leaf loft1by
the tree t2.
7Here, with “pump tree” we refer to partial derivation trees one adds or removes in the proof of the pumping lemma
for context-free grammars.

We proceed by induction on the number of nodes. In the base case, thas just one node, so d(t) = 0,
hence tis compact, and we are done. In the following, assume that thas more than one node and d(t)> L(t)
holds. We show how to construct a compact tree from t.
Let w.l.o.g. s1,s2,... ,s rbe the children of twithd(t)≥d(s1)≥d(s2)≥...≥d(sr). By induction we
can make every child compact, i.e. d(si)≤L(si). We then have by deﬁnition of dimension
L(t) + 1≤d(t)≤d(s1) + 1≤L(s1) + 1≤L(t) + 1.
Hence, we have d(t) =d(s1) + 1 which, by deﬁnition of dimension and compactness, implies d(s1) =d(s2) =
L(t) =L(s1) =L(s2). As h(s2)≥d(s2) =L(s2) by the remark after Deﬁnition 6.7, we ﬁnd a path in s2
from the root to a leaf which passes through at least two nodes with the same λv-label, say Xj. In other
words, we may factor s2intotb
1·(tb
2·tb
3) such that λv(tb
2) =λv(tb
3) =Xj. AsL(t) =L(s1) =L(s2), we also
ﬁnd a node of s1labelled by Xjwhich allows us to write s1=ta
1·ta
2withλv(ta
2) =Xj.
Now we move the middle part of s2tos1, i.e., let s′
1=ta
1·(tb
2·ta
3) and let s′
2=tb
1·tb
3. We then have
L(s′
1) =L(s1) =L(s2)≥L(s′
2). By induction, s′
1ands′
2can be made compact, so d(s′
1)≤d(s1) =d(s2)≥
d(s′
2). Consider the tree t′obtained from tby replacing s1bys′
1ands2bys′
2. By commutativity, tandt′
have the same yield. If d(s′
2)< d(s2) then d(t′)≤d(t)−1 =L(t) =L(t′) and we are done. Otherwise we
iterate the described procedure.
This procedure terminates, because the number of nodes of (the current) s2strictly decreases in every
iteration, and the number of nodes is an upper bound for h(s2) and, therefore, for d(s2). ⊓ ⊔
Now we can prove the main theorem of this section.
Theorem 7.7. Letfbe a vector of power series over a ci-semiring Sgiven in the set Xof variables with
|X|=n. Then ν(n)=µf.
Proof. We have for all X∈ X:
(µf)X=/summationdisplay
trees twith λv(t)=XY(t) (Corollary 6.6)
=/summationdisplay
trees twith λv(t)=X
andd(t)≤nY(t) (Lemma 7.5)
= (ν(n))X (Theorem 6.8) ⊓ ⊔
Remark 7.8. The bound of this theorem is tight, as shown by the following example: If f(X1,... ,X n) =
(X2
2+a,X2
3,... ,X2
n,X2
1), then ( ν(k))X1=afork < n, buta2n≤(ν(n))X1= (µf)X1.
8 Non-Distributive Program Analyses
In this paper we have focused on distributive program analyses, which allows us to use semirings as algebraic
structure. Recall that semirings are distributive, i.e., all semiring elements a,b,c satisfy a·(b+c) =a·b+a·c
and (a+b)·c=a·c+b·c.
Distributive intraprocedural analyses (i.e., for programs without procedures) were considered ﬁrst in
[Kil73]. This seminal paper showed that, given a program and the distributive t ransfer functions of a program
analysis, one can construct a vector fof polynomials such that, for every program point p, thep-component
of the least ﬁxed point µfcoincides with the JOP0-value , i.e., the join over all valid paths where for every
procedure call there is a matching return (as described in the introduction 1.1).
The framework of [Kil73] was generalized to non-distributive transfer functions i n [KU77]. Non-
distributivity means, in our terms, that only subdistributivity holds: a·(b+c)⊒a·b+a·cand
(a+b)·c⊒a·c+b·c.8There are interesting program analyses, such as constant propagation, which
8If addition is idempotent (as for lattice joins) this condition is equi valent to the monotonicity of multiplication, or,
in traditional terms, to the monotonicity of the transfer functions [K U77]. The stricter distributivity condition, on
the other hand, amounts to requiring the transfer functions to be hom omorphisms.

are non-distributive, see e.g. [KU77,NNH99]. In those cases, the least ﬁxed poi nt does not necessarily coin-
cide with the JOP 0-value, but rather safely approximates (“overapproximates”) it.
[SP81] extended the work of [Kil73] to the interprocedural case. The generalization t o non-distributive
analyses was done by [KS92], who proved that, as in the intraprocedural case, the lea st ﬁxed point is an
overapproximation of the JOP 0-value.
We deﬁne the JOP 0-value as the vector MwithMp=Y(Tp), where Tpis the set of trees labeled with p.
Notice that a depth-ﬁrst traversal of a tree labeled with pprecisely corresponds to an interprocedural path
from the beginning of the procedure of pto the program point p, i.e., the JOP 0-value Mp=Y(Tp) is indeed
the sum of the dataﬂow values of all paths to p. Corollary 6.6 states that M=µfholds in the distributive
case. Proposition 2.4 and Theorem 3.9 show that the Kleene and Newton sequences converg e to this value.
For the non-distributive case, the least ﬁxed point overapproximates the JOP 0-value, i.e., M⊑µf, cf.
[KS92].
In the following we show that Newton’s method is still well-deﬁned in “sub-distribut ive semirings”, and
that the Kleene and Newton sequences both converge to overapproximations of M, more precisely, we show
M⊑supi∈Nκ(i)⊑supi∈Nν(i).
For this we ﬁrst deﬁne subdistributive ( ω-complete) semirings9:
Deﬁnition 8.1. A subdistributive semiring is a tuple /a\}bracke⊔le{⊔S,+,·,0,1/a\}bracke⊔ri}h⊔satisfying the following properties:
1./a\}bracke⊔le{⊔S,+,0/a\}bracke⊔ri}h⊔is a commutative monoid.
2./a\}bracke⊔le{⊔S,·,1/a\}bracke⊔ri}h⊔is a monoid.
3.0·a=a·0 = 0 for all a∈S.
4.a·(b+c)⊒a·b+a·cand(a+b)·c⊒a·c+b·cfor all a,b,c ∈S.
5. The relation ⊑:={(a,b)∈S×S| ∃d∈S:a+d=b}is a partial order.
6. For all ω-chains ( ai)i∈N(i.e.a0⊑a1⊑a2⊑...withai∈S)sup⊑
i∈Naiexists. For anysequence (bi)i∈N
deﬁne/summationtext
i∈Nbi:= sup⊑{b0+b1+...+bi|i∈N}.
Remark 8.2. We obtain the deﬁnition of subdistributive semiring from the deﬁnition of ω-continuous semiring
by removing (7), and replacing distributivity with subdistributivity (see ( 4)).
In the rest of the section /a\}bracke⊔le{⊔S,+,·,0,1/a\}bracke⊔ri}h⊔denotes a subdistributive semiring. Polynomials, vectors, diﬀerential,
etc. are deﬁned as in the distributive setting.
Note that the following inequalities still hold for all sequences ( ai)i∈N,c∈S, and partitions ( Ij)j∈JofN:
c·/parenleftBigg/summationdisplay
i∈Nai/parenrightBigg
⊒/summationdisplay
i∈N(c·ai),/parenleftBigg/summationdisplay
i∈Nai/parenrightBigg
·c⊒/summationdisplay
i∈N(ai·c),/summationdisplay
j∈J
/summationdisplay
i∈Ijaj
⊒/summationdisplay
i∈Nai.
Thus, any polynomial pis still monotone, although not necessarily ω-continuous. For any sequence ( vi)i∈N
(of vectors) we still have p(/summationtext
i∈Nvi)⊒/summationtext
i∈Np(vi). Hence, the Kleene sequence of a polynomial system f
still converges, but not necessarily to the least ﬁxed point of f:
Corollary 8.3. For any system fof polynomials, the Kleene sequence (κ(i))i∈Nis an ω-chain. Moreover,
iffhas a least solution µf, then supi∈Nκ(i)⊑µf.
Since the Kleene sequence is still an ω-chain, its limit exists and is a safe approximation of the JOP 0-value:
Proposition 8.4. For any polynomial system fwe have/parenleftbig
κ(i)/parenrightbig
X⊒Y(Hi
X), and, hence,/parenleftbig
supi∈Nκ(i)/parenrightbig
X⊒
Y(TX)where TXis the set of trees labeled with X.
We skip the proof of this proposition as it is almost identical to the one of Proposition 6.4. The only diﬀerence
is that when expanding the components of κ(i)into a sum of products of coeﬃcients, subdistributivity only
guarantees that κ(i)is an upper bound, but not equality anymore. Similarly, subdistributivity only a llows
us to generalize the lower bound from Lemma 5.4, i.e. we have
f(u) +Df|u(v)⊑f(u+v)
for a polynomial system fand vectors u,v.
We now turn to the deﬁnition of Newton sequence .
9We drop ω-complete in the following.

Deﬁnition 8.5. Forfa polynomial system in the variables X, and a,bvectors we set
Lf;a;b(X) :=b+Df|a(X).
Deﬁnition 8.6. Letfbe a polynomial system.
–Leti∈N. Ani-thNewton approximant ν(i)is inductively deﬁned by
ν(0)=f(0)and ν(i+1)=ν(i)+∆(i),
where ∆(i)has to satisfy/summationtext
k∈NDf|k
ν(i)(δ(i))⊑∆(i)⊑Lf;ν(i);δ(i)/parenleftbig
∆(i)/parenrightbig
.
–Any such sequence (ν(i))i∈Nof Newton approximants is called Newton sequence .
Remark 8.7. Ifδ(i)exists, then possible choices for ∆(i)are
/summationdisplay
k∈NDf|k
ν(i)(δ(i)),sup
k∈NLk
f;ν(i);δ(i)(0) or (if it exists) µLf;ν(i);δ(i).
Note that in the distributive setting all three values coincide.
Proposition 8.8. Letf:V→Vbe a vector of power series.
–For every Newton approximant ν(i)there exists a vector δ(i)such that f(ν(i)) =ν(i)+δ(i). So there is
at least one Newton sequence.
–Every Newton sequence ν(i)satisﬁes κ(i)⊑ν(i)⊑f(ν(i))⊑ν(i+1)for all i∈N.
Proof. First we prove for all i∈Nthat a suitable δ(i)exists and, at the same time, that the inequality
κ(i)⊑ν(i)⊑f(ν(i)) holds. We proceed by induction on i. For the base case i= 0 we have:
ν(0)=f(0) =κ(0)⊑κ(1)=f(κ(0)) =f(ν(0)).
So, there exists a δ(0)withν(0)+δ(0)=f(ν(0)), and hence we have:
ν(1)=ν(0)+∆(0)⊒ν(0)+/summationdisplay
k∈NDf|k
ν(0)(δ(0))⊒ν(0)+δ(0)=f(ν(0)).
For the induction step, let i≥0.
κ(i+1)=f(κ(i))⊑f(ν(i)) =ν(i)+δ(i)⊑ν(i)+/summationdisplay
k∈NDf|k
ν(i)(δ(i)).
As we require that/summationtext
k∈NDf|k
ν(i)(δ(i))⊑∆(i), it now immediately follows that
κ(i+1)⊑ν(i)+∆(i)=ν(i+1).
By deﬁnition of ∆(i)we have ∆(i)⊑Lf;ν(i);δ(i)(∆(i)), it therefore follows:
ν(i+1)=ν(i)+∆(i)⊑ν(i)+δ(i)+Df|ν(i)/parenleftbig
∆(i)/parenrightbig
=f(ν(i)) +Df|ν(i)/parenleftbig
∆(i)/parenrightbig
⊑f/parenleftbig
ν(i)+∆(i)/parenrightbig
=f(ν(i+1)).
We complete our proof by
f(ν(i+1)) =ν(i+1)+δ(i+1)⊑ν(i+1)+/summationdisplay
k∈NDf|k
ν(i+1)(δ(i+1))
⊑ν(i+1)+∆(i+1)=ν(i+2).
Proposition 8.9. LetMbe the JOP 0-value, i.e., the vector MwithMX=Y(TX). Then M⊑
supi∈Nκ(i)⊑supi∈Nν(i).

Proof. Follows directly from Propositions 8.4 and 8.8. ⊓ ⊔
Proposition 8.10. For∆(i)=/summationtext
k∈NDf|k
ν(i)(δ(i))we have supi∈Nν(i)⊑µf, ifµfexists.
Proof. The proof is almost identical to the one of Proposition 5.1. Note that t he proof of Lemma 5.5 does
not use distributivity.
Theorem 8.11 (Tree Characterization of the Newton Sequence). Let(ν(i))i∈Nbe a Newton sequence
off. For every X∈ Xand every i≥0we have/parenleftbig
ν(i)/parenrightbig
X⊒Y(Di
X), i.e., the X-component of the i-th Newton
approximant is a safe approximation of the yield of Di
X.
Proof. In the distributive setting we proved this theorem via induction where we expanded the the terms
we obtained using distributivity. In the subdistributive case the same proof s till guarantees that/parenleftbig
ν(i)/parenrightbig
X⊒
Y(Di
X).
9 Conclusions
Since its inception, the theory of program analysis has been based on two fundamental o bservations:
–Analysis problems can be reduced (using abstract interpretation [CC77]) to the m athematical problem
of computing the least solution of a system of equations over a semilattice.
–Such systems of equations can be solved using Kleene’s ﬁxed-point theorem as basic algo rithm scheme.
In this paper we have contributed to both of these points. On the one hand, we general ize the algebraic setting
from semilattices to arbitrary semirings (a generalization to idempotent semirings was already present in the
work of [RSJM05] on pushdown systems for program analysis). On the other hand, we obtain a new method
for solving the dataﬂow equations by generalizing Newton’s method to semirings.
The conceptually simple step from semilattices to semirings leads to a common al gebraic setting for
“qualitative” analyses (which, loosely speaking, explore the existence of execut ion paths satisfying a given
property) and “quantitative” analyses (in which paths are assigned a numeri cal weight, and one is interested
in the sum of the weights of all paths satisfying the property). Classical examples of qualitative analyses are
live variables, constant propagation, or alias analysis, while examples of quantitative analysis arise in the
study of probabilistic programs: probability of termination, expected execution time or, in the interprocedural
case, expected stack height (for the latter, see [EKM05,BEK05]). The common s etting allows us to compare
the algorithmic schemes used in the qualitative and quantitative case, and ex amine if a transfer of techniques
is possible. We have shown that Newton’s method can be generalized to the abstract setting. In particular,
it can be applied to qualitative analysis problems.
We have explored Newton’s method for idempotent semirings, i.e., for the semir ings corresponding to
qualitative analyses. We have shown that the beautiful algebraic algorit hm of [HK99] for solving systems
of equations over commutative Kleene algebras is a particular instance of Newton’s method. Moreover, we
have proved that the algorithm requires at most niterations for a system of nequations, a tight bound
that improves on the O(3n) bound presented in [HK99]. From a theoretical point of view, giving a purely
algebraic proof of this fact along the lines of [HK99,AEI01] is an interest ing challenge.
While this paper imports notions of calculus and numerical mathematics into prog ram analysis, our work
also has some consequences pointing in the opposite direction. Quantitative anal yses lead to systems of equa-
tions over the real semiring, a particular case of the systems over the real ﬁeld. Sur prisingly, the performance
of Newton’s method in this special case seems not to have received much attention f rom numerical mathe-
maticians. The method turns out to have much better properties than in the general cas e. A consequence
of our main result (which was already proved, in a slightly more restricted form, by [EY09]), is that on the
real semiring Newton’s method always converges to the least ﬁxed point start ing from zero. This is not so in
the real ﬁeld, where it may not converge or converge only locally, i.e., when star ted suﬃciently close to the
zero (see e.g. [Ort72,OR70]). In related work we have shown that the convergence order of the method is at
least linear, meaning that the number of accurate bits of the Newton approximants grows at least linearly
with the number of iterations [KLE07,EKL08,EKLBP].

APPENDIX
A Proofs of Section 6
To avoid typographical clutter in the following proofs, we use the follow ing notation. Given some class of
objects (e.g. derivation trees t) and a predicate P(t), we write
/summationdisplay
tY(t) :P(t)
instead of/summationdisplay
tsuch that P(t) holdsY(t).
Proposition 6.4./parenleftbig
κ(i)/parenrightbig
X=Y(Hi
X), i.e., the X-component of the i-th Kleene approximant κ(i)is equal
to the yield of Hi
X.
Proof. By induction on i. The base case i= 0 is easy. Induction step ( i≥0):
/parenleftbig
κ(i+1)/parenrightbig
X
=fX(κ(i))
=/summationdisplay
j∈JmX,j(κ(i))
=/summationdisplay
j∈Jy:/braceleftbiggmX,j=a1X1· · ·Xkak+1
y=a1κ(i)
X1· · ·κ(i)
Xkak+1
by induction:
=/summationdisplay
j∈Jy:/braceleftbiggmX,j=a1X1· · ·Xkak+1
y=a1Y(Hi
X1)· · ·Y(Hi
Xk)ak+1
=/summationdisplay
j∈J
t1,...,t ky:

mX,j=a1X1· · ·Xkak+1
t1,... ,t ktrees with h(tr)≤i,λv(tr) =Xr(1≤r≤k)
y=a1Y(t1)· · ·Y(tk)ak+1
=/summationdisplay
j∈J,tY(t) :tis a tree with h(t)≤i+ 1, λ(t) = (X,j)
=Y(Hi
X) ⊓ ⊔
The following deﬁnition of ﬁne dimension is analogous to Deﬁnition 6.7, but adds a second component,
which measures the length of the path from the root to the lowest node with the sa me dimension as the root:
Deﬁnition A.1 ((ﬁne dimension)). Theﬁne dimension dl(t) = (d(t),l(t))of a tree tis inductively deﬁned
as follows:
1. Ifthas no children, then dl (t) = (0 ,0).
2. Ifthas exactly one child t1, then dl (t) = (d(t1),l(t1) + 1).
3. Ifthas at least two children, let t1,t2be two distinct children of tsuch that d(t1)≥d(t2)andd(t2)≥d(t′)
for every child t′/\e}a⊔io\slash=t1. Letd1=d(t1)andd2=d(t2). Then
dl(t) =/braceleftbigg(d1+ 1,0) ifd1=d2
(d1,l(t1) + 1) ifd1> d2.

Remark A.2. Notice that, by Deﬁnition 6.9, a tree tis proper if and only if l(t) = 0. So we have:
Y(Pi
X) =/summationdisplay
tY(t) :ttree with λv(t) =X,dl(t) = (i,0)
Now we can prove the remaining lemmata from Section 6.
Lemma 6.10. For every variable X∈ Xand every i≥0:τ(i)
X=Y(Di
X).
Proof. By induction on i. Induction base ( i= 0):
τ(0)
X=fX(0) =/summationdisplay
tY(t) :λv(t) =X,h(t) = 0
=/summationdisplay
tY(t) :λv(t) =X,d(t) = 0
=Y(D0
X)
Induction step ( i+ 1>0):
We need to show that Df|∗
τ(i)(δ(i)) equals exactly the yield of all trees of dimension i+ 1, i.e., that for all
X∈ X /parenleftBig
Df|∗
τ(i)(δ(i))/parenrightBig
X=/summationdisplay
tY(t) :λv(t) =X, d(t) =i+ 1.
We prove the following stronger claim by induction on p:
/parenleftBig
Df|p
τ(i)(δ(i))/parenrightBig
X=/summationdisplay
tY(t) :λv(t) =X,dl(t) = (i+ 1,p)
The claim holds for p= 0 by Remark A.2. For the induction step, let p≥0. Then we have for all X∈ X:
/parenleftBig
Df|p+1
τ(i)(δ(i))/parenrightBig
X
=/parenleftBig
Df|τ(i)◦Df|p
τ(i)(δ(i))/parenrightBig
X
=DfX|τ(i)◦Df|p
τ(i)(δ(i))
Deﬁne the vector /tildewideYby/tildewideYX0=/summationtext
tY(t) :λv(t) =X0,dl(t) = (i+ 1,p).Then, by induction hypothesis (on
p), above expression equals
=DfX|τ(i)(/tildewideY)
=/summationdisplay
j∈JDmX,j|τ(i)(/tildewideY) :mX,j=a1X1· · ·akXkak+1
=/summationdisplay
j∈J,ry:

mX,j=a1X1· · ·akXkak+1
1≤r≤k
y=a1τ(i)
X1· · ·ar/tildewideYXrar+1τ(i)
Xr+1· · ·akτ(i)
Xkak+1
by induction on i:
=/summationdisplay
j∈J,r,
t1,...,t ky:

mX,j=a1X1· · ·akXkak+1
1≤r≤k
t1,... ,t ktrees with λv(ts) =Xs(1≤s≤k)
dl(tr) = (i+ 1,p),
d(ts)≤i(1≤s≤k, s/\e}a⊔io\slash=r)
y=a1Y(t1)· · ·arY(tr)· · ·akY(tk)ak+1

=/summationdisplay
j∈J,tY(t) :ttree with λ(t) = (X,j),dl(t) = (i+ 1,p+ 1)
=/summationdisplay
tY(t) :ttree with λv(t) =X,dl(t) = (i+ 1,p+ 1) ⊓ ⊔
Lemma 6.11. The sequence (τ(i))i∈Nis a Newton sequence as deﬁned in Deﬁnition 3.6, i.e., the δ(i)of
Deﬁnition 6.9 satisfy f(τ(i)) =τ(i)+δ(i).
Proof.
fX(τ(i)) =/summationdisplay
j∈JmX,j(τ(i))
=/summationdisplay
j∈Jy:/braceleftbiggmX,j=a1X1· · ·akXkak+1
y=a1τ(i)
X1· · ·akτ(i)
Xkak+1
by Lemma 6.10:
=/summationdisplay
j∈J
t1,...,t ky:

mX,j=a1X1· · ·akXkak+1
t1,... ,t ktrees with λv(tr) =Xr, d(tr)≤i,(1≤r≤k)
y=a1Y(t1)· · ·akY(tk)ak+1
=/summationdisplay
j∈J
t1,...,t ky:

mX,j=a1X1· · ·akXkak+1
t1,... ,t ktrees with λv(tr) =Xr, d(tr)≤i,(1≤r≤k)
such that at most one of the trwithd(tr) =i
y=a1Y(t1)· · ·akY(tk)ak+1
+/summationdisplay
j∈J
t1,...,t ky:

mX,j=a1X1· · ·akXkak+1
t1,... ,t ktrees with λv(tr) =Xr, d(tr)≤i,(1≤r≤k)
such that at least two of the trwithd(tr) =i
y=a1Y(t1)· · ·akY(tk)ak+1
=/summationdisplay
tY(t) :ttree with λv(t) =X, d(t)≤i
+/summationdisplay
tY(t) :ttree with λv(t) =X,dl(t) = (i+ 1,0)
by Lemma 6.10 resp. Remark A.2:
=τ(i)
X+Y(Pi+1
X)
=τ(i)
X+δ(i)
X ⊓ ⊔
Acknowledgment
We thank Helmut Seidl and the anonymous referees for helpful suggestions and remarks.
References
[AEI01] Luca Aceto, Zolt´ an ´Esik, and Anna Ing´ olfsd´ ottir. A fully equational proof of Parikh’s th eorem. RAIRO,
Theoretical Informatics and Applications , 36:200–2, 2001.
[BEK05] Tom´ as Br´ azdil, Javier Esparza, and Anton´ ın Kucera. Analysis and prediction of the long-run behavior of
probabilistic sequential programs with recursion. In Proceedings of FOCS , pages 521–530. IEEE, 2005.
[Brz64] Janusz A. Brzozowski. Derivatives of regular expressions. Journal of the ACM , 11(4):481–494, 1964.

[CC77] P. Cousot and R. Cousot. Abstract interpretation: a uniﬁed lattic e model for static analysis of programs
by construction or approximation of ﬁxpoints. In Proceedings of POPL , pages 238–252. ACM, 1977.
[Deu94] Alain Deutsch. Interprocedural may-alias analysis for pointe rs: Beyond k-limiting. In Proceedings of PLDI ,
pages 230–241, 1994.
[EKL08] Javier Esparza, Stefan Kiefer, and Michael Luttenberger. Con vergence thresholds of Newton’s method for
monotone polynomial equations. In Proceedings of STACS , pages 289–300, 2008.
[EKLBP] J. Esparza, S. Kiefer, and M. Luttenberger. Computing the least ﬁxed point of positive polynomial systems.
SIAM Journal on Computing , TBP. To appear.
[EKM04] J. Esparza, A. Kuˇ cera, and R. Mayr. Model checking probabili stic pushdown automata. In Proceedings of
LICS, pages 12–21. IEEE, 2004.
[EKM05] J. Esparza, A. Kuˇ cera, and R. Mayr. Quantitative analysis of prob abilistic pushdown automata: Expecta-
tions and variances. In Proceedings of LICS , pages 117–126. IEEE, 2005.
[EY09] K. Etessami and M. Yannakakis. Recursive markov chains, stochas tic grammars, and monotone systems
of nonlinear equations. Journal of the ACM , 56(1):1–66, 2009.
[HK99] M. W. Hopkins and D. Kozen. Parikh’s theorem in commutative Klee ne algebra. In Proceedings of LICS ,
pages 394–401, 1999.
[JM82] N. Jones and S. Muchnick. A ﬂexible approach to interprocedur al data ﬂow analysis and programs with
recursive data structures. In Proceedings of POPL , pages 66–74. ACM, 1982.
[Kil73] G. A. Kildall. A uniﬁed approach to global program optimization. In Proceedings of POPL , pages 194–206.
ACM, 1973.
[KLE07] S. Kiefer, M. Luttenberger, and J. Esparza. On the convergenc e of Newton’s method for monotone systems
of polynomial equations. In Proceedings of STOC , pages 217–226. ACM, 2007.
[KS92] J. Knoop and B. Steﬀen. The interprocedural coincidence the orem. In International Conference on Com-
piler Construction , volume 641 of LNCS , pages 125–140. Springer-Verlag, 1992.
[KU77] J. B. Kam and J. D. Ullman. Monotone data ﬂow analysis frameworks. Acta Inf. , 7:305–317, 1977.
[Kui97] W. Kuich. Handbook of Formal Languages , volume 1, chapter 9: Semirings and Formal Power Series: Their
Relevance to Formal Languages and Automata, pages 609 – 677. Springer, 1997.
[NNH99] F. Nielson, H.R. Nielson, and C. Hankin. Principles of Program Analysis . Springer, 1999.
[OR70] J.M. Ortega and W.C. Rheinboldt. Iterative solution of nonlinear equations in several variables . Academic
Press, 1970.
[Ort72] J.M. Ortega. Numerical Analysis: A Second Course . Academic Press, New York, 1972.
[RHS95] T. Reps, S. Horwitz, and M. Sagiv. Precise interprocedural d ataﬂow analysis via graph reachability. In
Proceedings of POPL , pages 49–61. ACM, 1995.
[RSJM05] T. Reps, S. Schwoon, S. Jha, and D. Melski. Weighted pushdo wn systems and their application to inter-
procedural dataﬂow analysis. Science of Computer Programming , 58(1–2):206–263, October 2005. Special
Issue on the Static Analysis Symposium 2003.
[SF00] H. Seidl and C. Fecht. Interprocedural analyses: A comparison. Journal of Logic Programming (JLP ,
43:123–156, 2000.
[SP81] M. Sharir and A. Pnueli. Program Flow Analysis: Theory and Applications , chapter 7: Two Approaches to
Interprocedural Data Flow Analysis, pages 189–233. Prentice-Hall, 1981.
[SRH96] S. Sagiv, T. W. Reps, and S. Horwitz. Precise interprocedur al dataﬂow analysis with applications to
constant propagation. Theoretical Computer Science , 167(1&2):131–170, 1996.

